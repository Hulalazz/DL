- [Science of  Learning](#science-of--learning)
  - [Resource  on Deep Learning Theory](#resource--on-deep-learning-theory)
    - [Blogs and Paper](#blogs-and-paper)
    - [Course on Deep Learning](#course-on-deep-learning)
    - [Deep Learning Reading Group](#deep-learning-reading-group)
    - [Workshops](#workshops)
    - [Labs](#labs)
  - [Interpretability in AI](#interpretability-in-ai)
    - [Interpretability of Neural Networks](#interpretability-of-neural-networks)
    - [DeepLEVER](#deeplever)
    - [DLphi](#dlphi)
    - [Scientific Machine Learning](#scientific-machine-learning)
  - [Physics and Deep Learning](#physics-and-deep-learning)
    - [Machine Learning for Physics](#machine-learning-for-physics)
      - [Deep Learning for Physics](#deep-learning-for-physics)
    - [Physics for Machine Learning](#physics-for-machine-learning)
      - [Physics Informed Machine Learning](#physics-informed-machine-learning)
      - [Physics Informed Deep Learning](#physics-informed-deep-learning)
    - [Statistical Mechanics and Deep Learning](#statistical-mechanics-and-deep-learning)
    - [Born Machine](#born-machine)
    - [Quantum Machine learning](#quantum-machine-learning)
  - [Mathematics of Deep Learning](#mathematics-of-deep-learning)
    - [Discrete Mathematics and  Neural Networks](#discrete-mathematics-and--neural-networks)
      - [MIP and Deep Learning](#mip-and-deep-learning)
    - [Numerical Analysis for Deep Learning](#numerical-analysis-for-deep-learning)
      - [ResNets](#resnets)
      - [Differential Equations Motivated Deep Learning Methods](#differential-equations-motivated-deep-learning-methods)
    - [Control Theory and Deep Learning](#control-theory-and-deep-learning)
    - [Neural Ordinary Differential Equations](#neural-ordinary-differential-equations)
  - [Dynamics and Deep Learning](#dynamics-and-deep-learning)
    - [Stability For Neural Networks](#stability-for-neural-networks)
  - [Differential Equation and Deep Learning](#differential-equation-and-deep-learning)
    - [Deep Learning for PDEs](#deep-learning-for-pdes)
    - [$\mathcal H$ matrix and deep learning](#mathcal-h-matrix-and-deep-learning)
    - [Stochastic Differential Equations and Deep Learning](#stochastic-differential-equations-and-deep-learning)
    - [Finite Element Methods and Deep Learning](#finite-element-methods-and-deep-learning)
  - [Approximation Theory for Deep Learning](#approximation-theory-for-deep-learning)
      - [Workshop](#workshop)
      - [Labs and Groups](#labs-and-groups)
    - [The F-Principle](#the-f-principle)
  - [Inverse Problem and Deep Learning](#inverse-problem-and-deep-learning)
    - [Deep Learning for Inverse Problems](#deep-learning-for-inverse-problems)
    - [Deep Inverse Optimization](#deep-inverse-optimization)
  - [Random Matrix Theory and Deep Learning](#random-matrix-theory-and-deep-learning)
    - [Nonlinear Random Matrix Theory](#nonlinear-random-matrix-theory)
  - [Deep learning and Optimal Transport](#deep-learning-and-optimal-transport)
    - [Generative Models and Optimal Transport](#generative-models-and-optimal-transport)
  - [Geometric Analysis Approach to AI](#geometric-analysis-approach-to-ai)
    - [Tropical Geometry of Deep Neural Networks](#tropical-geometry-of-deep-neural-networks)
  - [Topology and Deep Learning](#topology-and-deep-learning)
    - [Topology Optimization and  Deep Learning](#topology-optimization-and--deep-learning)
  - [Algebra and Deep Learning](#algebra-and-deep-learning)
    - [Tensor network](#tensor-network)
    - [Group Equivariant Convolutional Networks](#group-equivariant-convolutional-networks)
    - [Complex Valued Neural Networks](#complex-valued-neural-networks)
    - [Quaternion Neural Networks](#quaternion-neural-networks)
  - [Probabilistic Theory and Deep Learning](#probabilistic-theory-and-deep-learning)
    - [Bayesian Deep Learning](#bayesian-deep-learning)
  - [Statistics and Deep Learning](#statistics-and-deep-learning)
    - [Statistical Relational AI](#statistical-relational-ai)
    - [Principal Component Neural Networks](#principal-component-neural-networks)
    - [Least squares support vector machines](#least-squares-support-vector-machines)
  - [Information Theory and Deep Learning](#information-theory-and-deep-learning)
    - [Information bottleneck theory](#information-bottleneck-theory)
  - [Brain Science and AI](#brain-science-and-ai)
    - [Spiking neural networks](#spiking-neural-networks)
    - [The Thousand Brains Theory of Intelligence](#the-thousand-brains-theory-of-intelligence)
  - [Cognition Science and Deep Learning](#cognition-science-and-deep-learning)
  - [The lottery ticket hypothesis](#the-lottery-ticket-hypothesis)
  - [Double Descent](#double-descent)
  - [Neural Tangents](#neural-tangents)
# Science of  Learning

V. Vapnik said that ``Nothing is more practical than a good theory.''
Here we focus on the theoretical machine learning.

- [CONSTRAINT REASONING AND OPTIMIZATION](https://www.helsinki.fi/en/researchgroups/constraint-reasoning-and-optimization)
- https://www.math.ubc.ca/~erobeva/seminar.html
- https://www.deel.ai/theoretical-guarantees/
- http://www.vanderschaar-lab.com/NewWebsite/index.html
- https://nthu-datalab.github.io/ml/index.html
- http://www.cs.cornell.edu/~shmat/research.html
- http://www.prace-ri.eu/best-practice-guide-deep-learning
- https://math.ethz.ch/sam/research/reports.html?year=2019
- http://gr.xjtu.edu.cn/web/jjx323/home
- https://zhouchenlin.github.io/
- https://www.math.tamu.edu/~bhanin/
- https://yani.io/annou/
- https://probability.dmi.unibas.ch/seminar.html
- http://mjt.cs.illinois.edu/courses/dlt-f19/
- http://danroy.org/
- [Symbolic Methods for Biological Networks](https://www.symbiont-project.org/)
- https://losslandscape.com/faq/
- https://mcallester.github.io/ttic-31230/


Deep learning is a transformative technology that has delivered impressive improvements in image classification and speech recognition. 
Many researchers are trying to better understand how to improve prediction performance and also how to improve training methods. 
[Some researchers use experimental techniques; others use theoretical approaches.](https://stats385.github.io/)

- https://www.cl.cam.ac.uk/~rja14


[The development of a "Science of Deep Learning" is now an active, interdisciplinary area of research combining insights from information theory, statistical physics, mathematical biology, and others.](https://www.neuralnet.science/)
Deep learning is at least related with kernel tricks, projection pursuit and neural networks.
- [Science of  Learning](#science-of--learning)
  - [Resource  on Deep Learning Theory](#resource--on-deep-learning-theory)
    - [Blogs and Paper](#blogs-and-paper)
    - [Course on Deep Learning](#course-on-deep-learning)
    - [Deep Learning Reading Group](#deep-learning-reading-group)
    - [Workshops](#workshops)
    - [Labs](#labs)
  - [Interpretability in AI](#interpretability-in-ai)
    - [Interpretability of Neural Networks](#interpretability-of-neural-networks)
    - [DeepLEVER](#deeplever)
    - [DLphi](#dlphi)
    - [Scientific Machine Learning](#scientific-machine-learning)
  - [Physics and Deep Learning](#physics-and-deep-learning)
    - [Machine Learning for Physics](#machine-learning-for-physics)
      - [Deep Learning for Physics](#deep-learning-for-physics)
    - [Physics for Machine Learning](#physics-for-machine-learning)
      - [Physics Informed Machine Learning](#physics-informed-machine-learning)
      - [Physics Informed Deep Learning](#physics-informed-deep-learning)
    - [Statistical Mechanics and Deep Learning](#statistical-mechanics-and-deep-learning)
    - [Born Machine](#born-machine)
    - [Quantum Machine learning](#quantum-machine-learning)
  - [Mathematics of Deep Learning](#mathematics-of-deep-learning)
    - [Discrete Mathematics and  Neural Networks](#discrete-mathematics-and--neural-networks)
      - [MIP and Deep Learning](#mip-and-deep-learning)
    - [Numerical Analysis for Deep Learning](#numerical-analysis-for-deep-learning)
      - [ResNets](#resnets)
      - [Differential Equations Motivated Deep Learning Methods](#differential-equations-motivated-deep-learning-methods)
    - [Control Theory and Deep Learning](#control-theory-and-deep-learning)
    - [Neural Ordinary Differential Equations](#neural-ordinary-differential-equations)
  - [Dynamics and Deep Learning](#dynamics-and-deep-learning)
    - [Stability For Neural Networks](#stability-for-neural-networks)
  - [Differential Equation and Deep Learning](#differential-equation-and-deep-learning)
    - [Deep Learning for PDEs](#deep-learning-for-pdes)
    - [$\mathcal H$ matrix and deep learning](#mathcal-h-matrix-and-deep-learning)
    - [Stochastic Differential Equations and Deep Learning](#stochastic-differential-equations-and-deep-learning)
    - [Finite Element Methods and Deep Learning](#finite-element-methods-and-deep-learning)
  - [Approximation Theory for Deep Learning](#approximation-theory-for-deep-learning)
      - [Workshop](#workshop)
      - [Labs and Groups](#labs-and-groups)
    - [The F-Principle](#the-f-principle)
  - [Inverse Problem and Deep Learning](#inverse-problem-and-deep-learning)
    - [Deep Learning for Inverse Problems](#deep-learning-for-inverse-problems)
    - [Deep Inverse Optimization](#deep-inverse-optimization)
  - [Random Matrix Theory and Deep Learning](#random-matrix-theory-and-deep-learning)
    - [Nonlinear Random Matrix Theory](#nonlinear-random-matrix-theory)
  - [Deep learning and Optimal Transport](#deep-learning-and-optimal-transport)
    - [Generative Models and Optimal Transport](#generative-models-and-optimal-transport)
  - [Geometric Analysis Approach to AI](#geometric-analysis-approach-to-ai)
    - [Tropical Geometry of Deep Neural Networks](#tropical-geometry-of-deep-neural-networks)
  - [Topology and Deep Learning](#topology-and-deep-learning)
    - [Topology Optimization and  Deep Learning](#topology-optimization-and--deep-learning)
  - [Algebra and Deep Learning](#algebra-and-deep-learning)
    - [Tensor network](#tensor-network)
    - [Group Equivariant Convolutional Networks](#group-equivariant-convolutional-networks)
    - [Complex Valued Neural Networks](#complex-valued-neural-networks)
    - [Quaternion Neural Networks](#quaternion-neural-networks)
  - [Probabilistic Theory and Deep Learning](#probabilistic-theory-and-deep-learning)
    - [Bayesian Deep Learning](#bayesian-deep-learning)
  - [Statistics and Deep Learning](#statistics-and-deep-learning)
    - [Statistical Relational AI](#statistical-relational-ai)
    - [Principal Component Neural Networks](#principal-component-neural-networks)
    - [Least squares support vector machines](#least-squares-support-vector-machines)
  - [Information Theory and Deep Learning](#information-theory-and-deep-learning)
    - [Information bottleneck theory](#information-bottleneck-theory)
  - [Brain Science and AI](#brain-science-and-ai)
    - [Spiking neural networks](#spiking-neural-networks)
    - [The Thousand Brains Theory of Intelligence](#the-thousand-brains-theory-of-intelligence)
  - [Cognition Science and Deep Learning](#cognition-science-and-deep-learning)
  - [The lottery ticket hypothesis](#the-lottery-ticket-hypothesis)
  - [Double Descent](#double-descent)
  - [Neural Tangents](#neural-tangents)

## Resource  on Deep Learning Theory 

### Blogs and Paper


* [Understanding Neural Networks by embedding hidden representations](https://rakeshchada.github.io/Neural-Embedding-Animation.html)
* [Tractable Deep Learning](https://www.cs.washington.edu/research/tractable-deep-learning)
* [DALI 2018, Data Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
* [MATHEMATICS OF DEEP LEARNING, NYU, Spring 2018](https://github.com/joanbruna/MathsDL-spring18)
* [Theory of Deep Learning, project in researchgate](https://www.researchgate.net/project/Theory-of-Deep-Learning)
* [THE THEORY OF DEEP LEARNING - PART I](https://physicsml.github.io/blog/DL-theory.html)
* [Magic paper](http://cognitivemedium.com/magic_paper/index.html)
* [Principled Approaches to Deep Learning](https://www.padl.ws/)
* [A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/pdf/1811.03962.pdf)
* [Advancing AI through cognitive science](https://github.com/brendenlake/AAI-site)
* [Deep Learning and the Demand for Interpretability](http://stillbreeze.github.io/Deep-Learning-and-the-Demand-For-Interpretability/)
* [Doing the Impossible: Why Neural Networks Can Be Trained at All](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6052125/)
* [Deep Learning Drizzle](https://deep-learning-drizzle.github.io/)
* [A Comprehensive Analysis of Deep Regression](https://github.com/Stephlat/DeepRegression)
* [Deep Learning in Practice](https://www.lri.fr/~gcharpia/deeppractice/)
* [Deep learning theory](http://guillefix.me/cosmos/static/Deep%2520learning%2520theory)
* [The Science of Deep Learning](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_5058.html)
* [TBSI 2019 Retreat Conference](http://workshop.tbsi.edu.cn/index.html)
* http://principlesofdeeplearning.com/
* https://cbmm.mit.edu/education/courses
* [DALI 2018 - Data, Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
* [On Theory@http://www.deeplearningpatterns.com ](http://www.deeplearningpatterns.com/doku.php?id=theory)
* https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/85815724

-----
* https://zhuanlan.zhihu.com/p/22353056
* https://arxiv.org/abs/1909.13458

###  Course on Deep Learning


* [CoMS E6998 003: Advanced Topics in Deep Learning](https://www.advancedtopicsindeeplearning.com/)
* [Analyses of Deep Learning (STATS 385) 2019](https://stats385.github.io/)
* [Deep Learning Theory: Approximation, Optimization, Generalization](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html)
* [UVA DEEP LEARNING COURSE](https://uvadlc.github.io/)
* [Theories of Deep Learning (STATS 385)](https://stats385.github.io/)
* [Topics Course on Deep Learning for Spring 2016 by Joan Bruna, UC Berkeley, Statistics Department](https://github.com/joanbruna/stat212b)
* [Mathematical aspects of Deep Learning](http://elmos.scripts.mit.edu/mathofdeeplearning/)
* [MATH 6380p. Advanced Topics in Deep Learning Fall 2018](https://deeplearning-math.github.io/)
* [6.883 Science of Deep Learning: Bridging Theory and Practice -- Spring 2018](https://people.csail.mit.edu/madry/6.883/)
* [(Winter 2018) IFT 6085: Theoretical principles for deep learning](http://mitliagkas.github.io/ift6085-dl-theory-class/)
* [STAT 991: Topics in deep learning (UPenn)](https://github.com/dobriban/Topics-in-deep-learning)
### Deep Learning Reading Group

[yanjun](http://www.cs.virginia.edu//papers.htm) organized a wonderful reading group on deep learning.

- https://a2i2.deakin.edu.au/
- https://qdata.github.io/deep2Read/
- https://dlta-reading.github.io/
* http://www.mlnl.cs.ucl.ac.uk/readingroup.html
* https://labrosa.ee.columbia.edu/cuneuralnet/
* http://www.ub.edu/cvub/reading-group/
* https://team.inria.fr/perception/deeplearning/
* https://scholar.princeton.edu/csmlreading
* https://junjuew.github.io/elijah-reading-group/
* http://www.sribd.cn/DL/schedule.html
* http://lear.inrialpes.fr/people/gaidon/lear_xrce_deep_learning_01.html
* https://simons.berkeley.edu/events/reading-group-deep-learning
* https://csml.princeton.edu/readinggroup
* http://www.bicv.org/deep-learning/
* https://www.cs.ubc.ca/labs/lci/mlrg/
* https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
* https://project.inria.fr/deeplearning/
* https://hustcv.github.io/reading-list.html

### Workshops

* http://pwp.gatech.edu/fdl-2018/program/
* [Symposium Artificial Intelligence for Science, Industry and Society](https://indico.cern.ch/event/781223/)
* [4th Workshop on Semantic Deep Learning (SemDeep-4)](http://www.dfki.de/semdeep-4/)
* [TAU & GTDeepNet seminars](https://www.lri.fr/TAU_seminars/)
* [NeuroIP 2018 workshop on Deep Learning Theory](http://nips2018dltheory.rice.edu/)
* [Toward theoretical understanding of deep learning](https://unsupervised.cs.princeton.edu/deeplearningtutorial.html)
- [The Science of Deep Learning](http://www.nasonline.org/programs/nas-colloquia/completed_colloquia/science-of-deep-learning.html)
- [PG Program in Artificial Intelligence & Machine Learning: Business Applications](https://la.utexas.edu/greatlearning/uta-artificial-intelligence-machine-learning.html)
- [Explainable AI Workshop](https://www.nist.gov/news-events/events/2021/01/explainable-ai-workshop)
- https://people.eecs.berkeley.edu/~malik/
- [heoretical Foundation of Deep Learning (TFDL 2018)](https://sites.gatech.edu/fdl-2018/program/)
- https://simons.berkeley.edu/programs/dl2019
- https://www.minds.jhu.edu/tripods/
- https://humancompatible.ai/research
- [Theory of Deep Learning, ICML'2018](https://sites.google.com/site/deeplearningtheory/)
- [Identifying and Understanding Deep Learning Phenomena](http://deep-phenomena.org/)
- https://ijcai20interpretability.github.io/
- https://niceworkshop.org/
- https://ecea-5.sciforum.net/

### Labs

* https://ori.ox.ac.uk/labs/a2i/
- [NSF, Simons Foundation partner to uncover foundations of deep learning](https://www.nsf.gov/news/special_reports/announcements/082520.jsp)
- https://elsc.huji.ac.il/faculty-staff/haim-sompolinsky
- https://www.neuralnet.science/
- https://carolewu.engineering.asu.edu/
- https://www.nist.gov/artificial-intelligence
- https://humancompatible.ai/
- http://kordinglab.com/
- http://koerding.com/
- https://www.regina.csail.mit.edu/
- https://www.cs.huji.ac.il/~shashua/research.php
- http://www.cns.nyu.edu/~eero/
- https://xiangxiangxu.com/
* http://qszhang.com/index.php/team/
* https://gangwg.github.io/research.html
* http://www.mit.edu/~k2smith/
* https://www.msra.cn/zh-cn/news/people-stories/wei-chen
* https://www.microsoft.com/en-us/research/people/tyliu/
* https://www.researchgate.net/profile/Hatef_Monajemi

## Interpretability in AI 

* [2018 Workshop on Interpretable & Reasonable Deep Learning and its Applications (IReDLiA)](http://web.fsktm.um.edu.my/~cschan/iredlia.html)
- http://networkinterpretability.org/
- https://interpretablevision.github.io/
- https://vipriors.github.io/
- [Interpretability in AI and its relation to fairness, transparency, reliability and trust](https://ec.europa.eu/jrc/communities/en/node/1162/article/interpretability-ai-and-its-relation-fairness-transparency-reliability-and-trust)
- https://github.com/jphall663/awesome-machine-learning-interpretability
- https://people.mpi-sws.org/~manuelgr/
- [2nd HUMAINT Winter school on Fairness, Accountability and Transparency in Artificial Intelligence](https://ec.europa.eu/jrc/communities/en/community/humaint/event/2nd-humaint-winter-school-fairness-accountability-and-transparency)
- https://facctconference.org/network/
- https://calculatedcontent.com/

### Interpretability of Neural Networks

[ Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks.](http://academic.hep.com.cn/fitee/CN/10.1631/FITEE.1700808#1)
At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations.
We believe that high model interpretability may help people break several bottlenecks of deep learning,
e.g., learning from a few annotations, learning via human–computer communications at the semantic level,
and semantically debugging network representations.
We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations,
methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model interpretability.
Finally, we discuss prospective trends in explainable artificial intelligence.

- https://www.transai.org/
- [GAMES Webinar 2019 – 93期(深度学习可解释性专题课程) ](http://games-cn.org/games-webinar-20190509-93/)
- [GAMES Webinar 2019 – 94期(深度学习可解释性专题课程) | 刘日升（大连理工大学），张拳石（上海交通大学）](http://games-cn.org/games-webinar-20190516-94/)
- http://qszhang.com/index.php/publications/
- [Explaining Neural Networks Semantically and Quantitatively](https://arxiv.org/abs/1812.07169)
- https://www.jiqizhixin.com/articles/0211
- https://www.jiqizhixin.com/articles/030205
- https://mp.weixin.qq.com/s/xY7Cpe6idbOTJuyD3vwD3w
- http://academic.hep.com.cn/fitee/CN/10.1631/FITEE.1700808#1
- https://arxiv.org/pdf/1905.11833.pdf
- http://www.cs.sjtu.edu.cn/~leng-jw/
- https://lemondan.github.io
- http://ise.sysu.edu.cn/teacher/teacher02/1136886.htm
- http://www.cs.cmu.edu/~zhitingh/data/hu18texar.pdf
- https://datasciencephd.eu/DSSS19/slides/GiannottiPedreschi-ExplainableAI.pdf
- http://www.cs.cmu.edu/~zhitingh/
- https://graphreason.github.io/
* https://beenkim.github.io/
* https://www.math.ucla.edu/~montufar/
* [Explainable AI: Interpreting, Explaining and Visualizing Deep Learning](https://link.springer.com/book/10.1007/978-3-030-28954-6)
* http://www.prcv2019.com/en/index.html
* http://gr.xjtu.edu.cn/web/jiansun
* http://www.shixialiu.com/
* http://irc.cs.sdu.edu.cn/
* https://www.seas.upenn.edu/~minchenl/
* https://cs.nyu.edu/~yixinhu/
* http://www.cs.utexas.edu/~huangqx/
* https://stats385.github.io/

Not all one can understand the relative theory or quantum theory.

- [Interpretable Convolutional Neural Networks](https://github.com/zqs1022/interpretableCNN)
* [Integrated and detailed image understanding](https://www.robots.ox.ac.uk/~vedaldi//research/idiu/idiu.html)
### DeepLEVER

> DeepLEVER aims at explaining and verifying machine learning systems via combinatorial optimization in general and SAT in particular. 
[The main thesis of the DeepLever project](http://anitideeplever.laas.fr/project) is that a solution to address the challenges faced by ML models is at the intersection of formal methods (FM) and AI. (A recent Summit on Machine Learning Meets Formal Methods offered supporting evidence to how strategic this topic is.) The DeepLever project envisions two main lines of research, concretely explanation and verification of deep ML models, supported by existing and novel constraint reasoning technologies.

- [DeepLEVER](http://anitideeplever.laas.fr/deeplever-project-has-started)
- https://aniti.univ-toulouse.fr/index.php/en/
- https://jpmarquessilva.github.io/
- https://www.researchgate.net/profile/Martin_Cooper3
- http://homepages.laas.fr/ehebrard/Home.html
- http://www.merl.com/

### DLphi

> Together with the participants of the Oberwolfach Seminar: Mathematics of Deep Learning, [I wrote a (not entirely serious) paper](http://www.pc-petersen.eu/) called "The Oracle of DLPhi" proving that `Deep Learning techniques can perform accurate classifications on test data that is entirely uncorrelated to the training data`. This, however, requires a couple of non-standard assumptions such as uncountably many data points and the axiom of choice. In a sense this shows that mathematical results on machine learning need to be approached with a bit of scepticism.

- https://github.com/juliusberner/oberwolfach_workshop
- http://www.pc-petersen.eu/
- http://voigtlaender.xyz/
- https://math.ethz.ch/sam/research/reports.html
- [The Oracle of DLphi](https://arxiv.org/abs/1901.05744)
- https://faculty.washington.edu/kutz/

### Scientific Machine Learning  

[Scientific machine learning is a burgeoning discipline which blends scientific computing and machine learning. Traditionally, scientific computing focuses on large-scale mechanistic models, usually differential equations, that are derived from scientific laws that simplified and explained phenomena. On the other hand, machine learning focuses on developing non-mechanistic data-driven models which require minimal knowledge and prior assumptions. The two sides have their pros and cons: differential equation models are great at extrapolating, the terms are explainable, and they can be fit with small data and few parameters. Machine learning models on the other hand require "big data" and lots of parameters but are not biased by the scientists ability to correctly identify valid laws and assumptions.](https://thewinnower.com/papers/25359-the-essential-tools-of-scientific-machine-learning-scientific-ml)

- https://www.scd.stfc.ac.uk/Pages/Scientific-Machine-Learning.aspx
- https://mitmath.github.io/18337/
- https://www.stat.purdue.edu/~fmliang/STAT598Purdue/MLS.pdf
- https://sciml.ai/
- https://github.com/mitmath/18S096SciML
- https://ml4sci.lbl.gov/
- https://www.nottingham.ac.uk/conference/fac-sci/maths-sci/scientific-computation-using-machine-learning-algorithms/
- https://sites.google.com/lbl.gov/ml4sci/
- [SciANN: Neural Networks for Scientific Computations](https://github.com/sciann/sciann/)

## Physics and Deep Learning

Neuronal networks have enjoyed a resurgence both in the worlds of neuroscience, where they yield mathematical frameworks for thinking about complex neural datasets, and in machine learning, where they achieve state of the art results on a variety of tasks, including machine vision, speech recognition, and language translation.  
Despite their empirical success, a mathematical theory of how deep neural circuits, with many layers of cascaded nonlinearities, learn and compute remains elusive.  
We will discuss three recent vignettes in which ideas from statistical physics can shed light on this issue.  
In particular, we show how dynamical criticality can help in neural learning, how the non-intuitive geometry of high dimensional error landscapes can be exploited to speed up learning, and how modern ideas from non-equilibrium statistical physics, like the Jarzynski equality, can be extended to yield powerful algorithms for modeling complex probability distributions.  
[Time permitting, we will also discuss the relationship between neural network learning dynamics and the developmental time course of semantic concepts in infants.](https://physics.berkeley.edu/news-events/events/20151005/the-statistical-physics-of-deep-learning-on-the-beneficial-roles-of)

In recent years, artificial intelligence has made remarkable advancements, impacting many industrial sectors dependent on complex decision-making and optimization. 
Physics-leaning disciplines also face hard inference problems in complex systems: climate prediction, density matrix estimation for many-body quantum systems, material phase detection, protein-fold quality prediction, parametrization of effective models of high-dimensional neural activity, energy landscapes of transcription factor-binding, etc. 
Methods using artificial intelligence have in fact already advanced progress on such problems. 
[So, the question is not whether, but how AI serves as a powerful tool for data analysis in academic research, and physics-leaning disciplines in particular.](http://www.physics.mcgill.ca/ai2019/)

<img src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2017/09/InfoBottleneck_2880x1620.jpg" width="80%"/>

* https://julialang.org/jsoc/gsoc/sciml/
* https://zhuanlan.zhihu.com/p/94249675
* https://web.stanford.edu/~montanar/index.html
* [Physics Meets ML](https://www.microsoft.com/en-us/research/event/physics-ml-workshop/)
* [physics forests](http://apagom.com/physicsforests/)
* [Applied Machine Learning Days](https://www.appliedmldays.org/)
* [DEEP LEARNING FOR MULTIMESSENGER ASTROPHYSICS: REAL-TIME DISCOVERY AT SCALE](http://www.ncsa.illinois.edu/Conferences/DeepLearningLSST/)
* [Workshop on Science of Data Science | (smr 3283)](http://indico.ictp.it/event/8722/)
* [Physics & AI Workshop](http://www.physics.mcgill.ca/ai2019/)
* https://physicsml.github.io/pages/papers.html
* [Physics-AI opportunities at MIT](http://super-ms.mit.edu/physics-ai.html)
* https://gogul.dev/software/deep-learning-meets-physics
* https://github.com/2prime/ODE-DL/blob/master/DL_Phy.md
* https://physics-ai.com/
* http://physics.usyd.edu.au/quantum/Coogee2015/Presentations/Svore.pdf
* [Brains, Minds and Machines Summer Course](https://ocw.mit.edu/resources/res-9-003-brains-minds-and-machines-summer-course-summer-2015/index.htm)
* [deep medcine](http://amos3.aapm.org/abstracts/pdf/127-36916-419554-130797.pdf)
* http://www.dam.brown.edu/people/mraissi/publications/
* http://www.physics.rutgers.edu/gso/SSPAR/
* https://community.singularitynet.io/c/education/course-brains-minds-machines
* [ARTIFICIAL INTELLIGENCE AND PHYSICS](https://physai.sciencesconf.org/)
* http://inspirehep.net/record/1680302/references
* https://www.pnnl.gov/computing/philms/Announcements.stm
* https://tacocohen.wordpress.com/
* https://cnls.lanl.gov/External/workshops.php
* https://www.researchgate.net/profile/Jinlong_Wu3
* http://djstrouse.com/
* https://www.researchgate.net/scientific-contributions/2135376837_Maurice_Weiler
* [Spontaneous Symmetry Breaking in Neural Networks](https://arxiv.org/abs/1710.06096)
* https://physai.sciencesconf.org/


### Machine Learning for Physics

* [Deep Learning in High Energy Physics](https://dlonsc.github.io/ISC2019/7_Keynote_DL_HEP_SofiaVallecorsa.pdf)
* [Machine Learning for Physics and the Physics of Learning](https://www.ipam.ucla.edu/programs/long-programs/machine-learning-for-physics-and-the-physics-of-learning/)
* [Machine Learning for Physics](https://machine-learning-for-physicists.org/)
* [2017 Machine Learning for Physicists, by Florian Marquardt](http://www.thp2.nat.uni-erlangen.de/index.php/2017_Machine_Learning_for_Physicists,_by_Florian_Marquardt)
* [Machine Learning and the Physical Sciences](https://ml4physicalsciences.github.io/2020/)
* [Machine Learning in Physics School/Workshop](http://phys.cts.nthu.edu.tw/actnews/content.php?Sn=468)
* http://deeplearnphysics.org/

#### Deep Learning for Physics

* https://inspirehep.net/literature/1680302
* [Master-Seminar - Deep Learning in Physics (IN2107, IN0014)](https://www.in.tum.de/cg/teaching/winter-term-1819/deep-learning-in-physics/)
* https://www.ml4science.org/agenda-physics-in-ml
* https://www.ias.edu/events/deep-learning-physics
* https://dl4physicalsciences.github.io/

### Physics for Machine Learning

* https://tartakovsky.stanford.edu/research/physics-informed-machine-learning
* [Physics in Machine Learning Workshop](https://bids.berkeley.edu/events/physics-machine-learning-workshop)
* [Physics in Machine Learning Workshop](https://www.ml4science.org/astrophysics-in-machine-learning-workshop)
* [A Differentiable Physics Engine for Deep Learning](http://phys.csail.mit.edu/papers/1.pdf)
* [Physics Based Vision meets Deep Learning (PBDL)](https://pbdl2019.github.io/)
* [Physics-Based Deep Learning](https://github.com/thunil/Physics-Based-Deep-Learning)

#### Physics Informed Machine Learning

- [Physics Informed Machine Learning](https://tartakovsky.stanford.edu/research/physics-informed-machine-learning)
- https://www.pnnl.gov/computing/philms/
* https://sites.google.com/view/icml2019phys4dl/schedule
* [Theoretical Physics for Deep Learning](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3531)
* https://sites.google.com/view/icml2019phys4dl/schedule
* [Physics Informed Machine Learning Workshop](http://www.databookuw.com/page-5/)

#### Physics Informed Deep Learning

* [Physics Informed Neural Networks](https://github.com/maziarraissi/PINNs)
* [Physics Informed Deep Learning](https://maziarraissi.github.io/PINNs/)
* https://maziarraissi.github.io/research/1_physics_informed_neural_networks/
* https://github.com/maziarraissi/PINNs
* https://github.com/56aaaaa/Physics-informed-neural-networks
* [Physics-informed deep learning imaging](http://sites.bu.edu/tianlab/publications/physics-embedded-deep-learning/)
* https://github.com/DeepNeuralAI/DL-Physics-Neural-Network
  
### Statistical Mechanics and Deep Learning

[The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to shed conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.](https://www.annualreviews.org/doi/pdf/10.1146/annurev-conmatphys-031119-050745)

* [Statistical Physics of Machine Learning](https://www.icts.res.in/discussion-meeting/spmml2020)
* [statistical mechanics // machine learning](http://smml.io/)
* [A Theoretical Connection Between Statistical Physics and Reinforcement Learning](https://arxiv.org/abs/1906.10228)
* [The thermodynamics of learning](https://phys.org/news/2017-02-thermodynamics.html)
* [WHY DOES DEEP LEARNING WORK?](https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/)
* [WHY DEEP LEARNING WORKS II: THE RENORMALIZATION GROUP](https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/)
*  https://github.com/CalculatedContent/ImplicitSelfRegularization
* [torbenkruegermath](https://sites.google.com/site/torbenkruegermath/home/graduate-seminar-random-matrices-spin-glasses-deep-learning)
* [TOWARDS A NEW THEORY OF LEARNING: STATISTICAL MECHANICS OF DEEP NEURAL NETWORKS](https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/)
* [Statistical Mechanics of Deep Learning](https://www.annualreviews.org/doi/pdf/10.1146/annurev-conmatphys-031119-050745)
* https://zhuanlan.zhihu.com/p/90096775

### Born Machine

Born machine is a Probabilistic Generative Modeling.

- [Unsupervised Generative Modeling Using Matrix Product States](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031012#fulltext)
- https://wangleiphy.github.io/talks/BornMachine-USTC.pdf
- https://github.com/congzlwag/UnsupGenModbyMPS
- https://congzlwag.github.io/UnsupGenModbyMPS/
- https://github.com/congzlwag/BornMachineTomo
- [From Baltzman machine to Born Machine](https://wangleiphy.github.io/talks/BornMachine.pdf)
- [Born Machines: A fresh approach to quantum machine learning](https://quantum.ustc.edu.cn/web/node/623)
- [Gradient based training of Quantum Circuit Born Machine (QCBM)](https://github.com/GiggleLiu/QuantumCircuitBornMachine)

### Quantum Machine learning

[Quantum Machine Learning: What Quantum Computing Means to Data Mining explains the most relevant concepts of machine learning, quantum mechanics, and quantum information theory, and contrasts classical learning algorithms to their quantum counterparts.](https://peterwittek.com/)

- [Combining quantum information and machine learning](https://www.quantummachinelearning.org/events.html)
- [machine learning for quantum technology/](https://www.mpl.mpg.de/divisions/marquardt-division/workshops/2019-machine-learning-for-quantum-technology/)
- https://wangleiphy.github.io/
- https://tacocohen.wordpress.com
- https://peterwittek.com/qml-in-2015.html
- https://github.com/krishnakumarsekar/awesome-quantum-machine-learning
- https://peterwittek.com/
* [Lecture Note on Deep Learning and Quantum Many-Body Computation](https://wangleiphy.github.io/lectures/DL.pdf)
* [Quantum Deep Learning and Renormalization](http://www.math.chalmers.se/~stig/project4.pdf)

____
* https://scholar.harvard.edu/madvani/home
* https://www.elen.ucl.ac.be/esann/index.php?pg=specsess#statistical
* https://krzakala.github.io/cargese.io/program.html
* [New Theory Cracks Open the Black Box of Deep Learning](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)
* [Unifying Physics and Deep Learning with TossingBot](https://ai.googleblog.com/2019/03/unifying-physics-and-deep-learning-with.html)


## Mathematics of Deep Learning

- [Meeting on Mathematics of Deep Learning](https://www.4tu.nl/ami/en/Agenda-Events/)
- [Probability in high dimensions](http://www.yanivplan.com/math-608d)
- https://math.ethz.ch/sam/research/reports.html?year=2019
- [Learning Deep Learning](http://rt.dgyblog.com/ref/ref-learning-deep-learning.html)
- [Summer school on Deep Learning Theory by Weinan E](https://github.com/leiwu1990/course.math_theory_nn)
- [.520/6.860: Statistical Learning Theory and Applications, Fall 2018](http://www.mit.edu/~9.520/fall18/)
- [2018上海交通大学深度学习理论前沿研讨会 - 凌泽南的文章 - 知乎](https://zhuanlan.zhihu.com/p/40097048)
- [Theories of Deep Learning](https://www.researchgate.net/project/Theories-of-Deep-Learning)
- https://orion.math.iastate.edu/hliu/MDL/

A mathematical theory of deep networks and of why they work as well as they do is now emerging. 
[I will review some recent theoretical results on the approximation power of deep networks](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html) 
including conditions under which they can be exponentially better than shallow learning. 
A class of deep convolutional networks represent an important special case of these conditions, 
though weight sharing is not the main reason for their exponential advantage. 
I will also discuss another puzzle around deep networks: what guarantees that they generalize and
 they do not overfit despite the number of weights being larger than the number of training data and despite the absence of explicit regularization in the optimization?

Deep Neural Networks and Partial Differential Equations: Approximation Theory and
Structural Properties
Philipp Petersen, University of Oxford

- https://memento.epfl.ch/event/a-theoretical-analysis-of-machine-learning-and-par/
- http://at.yorku.ca/c/b/p/g/30.htm
- https://mat.univie.ac.at/~grohs/
- [Deep Learning: Theory and Applications (Math 689 Fall 2018)](https://www.math.tamu.edu/~bhanin/DL2018.html)
- [Topics course Mathematics of Deep Learning, NYU, Spring 18](https://joanbruna.github.io/MathsDL-spring18/)
- https://skymind.ai/ebook/Skymind_The_Math_Behind_Neural_Networks.pdf
- https://github.com/markovmodel/deeptime
- https://omar-florez.github.io/scratch_mlp/
- https://joanbruna.github.io/MathsDL-spring19/
- https://github.com/isikdogan/deep_learning_tutorials
- https://www.brown.edu/research/projects/crunch/machine-learning-x-seminars
- [Deep Learning: Theory & Practice](http://anotherdatum.com/tce_2018.html)
- https://www.math.ias.edu/wtdl
- https://www.ml.tu-berlin.de/menue/mitglieder/klaus-robert_mueller/
- https://www-m15.ma.tum.de/Allgemeines/MathFounNN
- https://www.math.purdue.edu/~buzzard/MA598-Spring2019/index.shtml
- http://mathematics-in-europe.eu/?p=801
- https://cims.nyu.edu/~bruna/
- https://www.math.ias.edu/wtdl
- https://www.pims.math.ca/scientific-event/190722-pcssdlcm
- [Deep Learning for Image Analysis EMBL COURSE](https://www.embl.de/training/events/2020/MAC20-01/)
- [MATH 6380o. Deep Learning: Towards Deeper Understanding, Spring 2018](https://deeplearning-math.github.io/2018spring.html)
- [Mathematics of Deep Learning, Courant Insititute, Spring 19](https://github.com/joanbruna/MathsDL-spring19)
- http://voigtlaender.xyz/
- http://www.mit.edu/~9.520/fall19/
- [The Mathematics of Deep Learning and Data Science - Programme](https://gateway.newton.ac.uk/event/ofbw46/programme)
+ [Home of Math + Machine Learning + X](https://www.brown.edu/research/projects/crunch/)
+ [Mathematical and Computational Aspects of Machine Learning](http://crm.sns.it/event/451/)
+ [Mathematical Theory for Deep Neural Networks](https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks)
+ [Theory of Deep Learning](https://www.researchgate.net/project/Theory-of-Deep-Learning)
+ [DALI 2018 - Data, Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
+ [BMS Summer School 2019: Mathematics of Deep Learning](https://www.math-berlin.de/academics/summer-schools/2019)
+ [SIAM Conference on Mathematics of Data Science (MDS20)](https://www.siam.org/conferences/cm/conference/mds20)
* http://web.cs.ucla.edu/~qgu/research.html
* [BRIDGING GAME THEORY AND DEEP LEARNING](https://sgo-workshop.github.io/)


### Discrete Mathematics and  Neural Networks


- [Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization](http://proceedings.mlr.press/v28/ermon13.html)
- [DATA SCIENCE MEETS OPTIMIZATION](https://sites.google.com/view/ijcai2019dso/)   
- http://www.cas.mcmaster.ca/~deza/tokyo2018progr.html
- https://www.cs.cornell.edu/~bistra/
- [Discrete Mathematics of Neural Networks: Selected Topics](https://epubs.siam.org/doi/book/10.1137/1.9780898718539?mobileUi=0)
- [Deep Learning in Computational Discrete Optimization](https://www.math.uwaterloo.ca/~bico/co759/2018/index.html)
- [Deep Learning in Discrete Optimization](http://www.ams.jhu.edu/~wcook12/dl/index.html)
- https://web-app.usc.edu/soc/syllabus/20201/30126.pdf
- http://www.columbia.edu/~yf2414/Slides.pdf
- http://www.columbia.edu/~yf2414/teach.html
- https://opt-ml.org/cfp.html
- https://easychair.org/smart-program/CPAIOR2020/index.html


#### MIP and Deep Learning

- [Strong mixed-integer programming formulations for trained neural networks by Joey Huchette1](http://www.cas.mcmaster.ca/~deza/slidesRIKEN2019/huchette.pdf)
- [Deep neural networks and mixed integer linear optimization](https://link.springer.com/article/10.1007/s10601-018-9285-6)
- [Matteo Fischetti, University of Padova](http://www.dei.unipd.it/~fisch/papers/slides/2018%20Dagstuhl%20%5BFischetti%20on%20DL%5D.pdf)
- [Deep Neural Networks as 0-1 Mixed Integer Linear Programs: A Feasibility Study](https://arxiv.org/abs/1712.06174)
- https://www.researchgate.net/profile/Matteo_Fischetti
- [A Mixed Integer Linear Programming Formulation to Artificial Neural Networks](http://www.amp.i.kyoto-u.ac.jp/tecrep/ps_file/2019/2019-001.pdf)
- [ReLU Networks as Surrogate Models in Mixed-Integer Linear Programs](http://www.optimization-online.org/DB_FILE/2019/07/7276.pdf)

### Numerical Analysis for Deep Learning

Dynamics of deep learning is to  consider deep learning as a dynamic system. 
For example, the forward feedback network is expressed in the recurrent form:
$$x^{t+1} = f_t(x^{t}),t\in [0,1,\cdots, T]$$
where $f_t$ is some nonlinear function and $t$ is discrete.

However, it is not easy to select a proper nonlinear function $f_t \,\,\forall t\in[0,1,\cdots, T]$ and the number $T$.
In another word, there are no unified scientific principle or  guide to design the structure of deep neural network models.  

Many recursive formula share the same `feedback` forms or hidden structure, where the next input is the output of previous or historical record or generated points.

- [401-3650-19L  Numerical Analysis Seminar: Mathematics of Deep Neural Network Approximation](http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?lerneinheitId=136996&semkez=2020S&lang=en)
- http://www.mathcs.emory.edu/~lruthot/talks/
- [CS 584 / MATH 789R - Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/math789r-sp20.html)
- [Numerical methods for deep learning](https://github.com/IPAIopen/NumDL-CourseNotes)
- [Short Course on Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)
- [MA 721: Topics in Numerical Analysis: Deep Learning](http://www.ms.uky.edu/~qye/MA721/ma721F17.html)
* [Physics-Based Deep Learning for Fluid Flow](http://phys2018.csail.mit.edu/papers/29.pdf)


#### ResNets

`Deep Residual Networks` won the 1st places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
It inspired more efficient forward  convolutional networks.

They take a standard feed-forward ConvNet and add skip connections that bypass (or shortcut) a few convolution layers at a time. Each bypass gives rise to a residual block in which the convolution layers predict a residual that is added to the block’s input tensor.

<img src="https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/resnets_1.png" width="40%"/>

- https://github.com/KaimingHe/deep-residual-networks
- http://torch.ch/blog/2016/02/04/resnets.html
- https://zh.gluon.ai/chapter_convolutional-neural-networks/resnet.html
- https://www.jiqizhixin.com/articles/042201
- http://www.smartchair.org/hp/MSML2020/Paper/
- https://github.com/liuzhuang13/DenseNet
- https://arxiv.org/abs/1810.11741
- [Depth with nonlinearity creates no bad local minima in ResNets](https://www.sciencedirect.com/science/article/pii/S0893608019301820?via%3Dihub)
- [LeanConvNets: Low-cost Yet Effective Convolutional Neural Networks](https://arxiv.org/abs/1910.13157)

**Reversible Residual Network**

- [The Reversible Residual Network: Backpropagation Without Storing Activations](https://arxiv.org/abs/1707.04585)
- https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html
- https://arxiv.org/abs/2001.04451
- https://ameroyer.github.io/reading-notes/architectures/2019/05/07/the_reversible_residual_network.html
- [Layer-Parallel Training of Deep Residual Neural Networks](https://arxiv.org/abs/1812.04352)

#### Differential Equations Motivated Deep Learning Methods

This section is on insight from numerical analysis to inspire more effective deep learning architecture.

[Many effective networks can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures.](https://web.stanford.edu/~yplu/proj/lm/)

 [We show that residual neural networks can be interpreted as discretizations of a nonlinear time-dependent ordinary differential equation that depends on unknown parameters, i.e., the network weights. We show how this insight has been used, e.g., to study the `stability of neural networks, design new architectures, or use established methods from optimal control methods for training ResNets`. Finally, we discuss open questions and opportunities for mathematical advances in this area.](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)


- [Path integral approach to random neural networks](https://elsc.huji.ac.il/all-publications/1050)
- [NEURAL NETWORKS AS ORDINARY DIFFERENTIAL EQUATIONS](https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/)
- [Dynamical aspects of Deep Learning](https://zhenyu-liao.github.io/pdf/pre/GDD_iCODE.pdf)
- [Dynamical Systems and Deep Learning](http://www.doc.ic.ac.uk/~ae/teaching.html#complex)
- https://zhuanlan.zhihu.com/p/71747175
- https://web.stanford.edu/~yplu/project.html
- https://github.com/2prime/ODE-DL/
- [Deep Neural Networks Motivated by Partial Differential Equations](https://arxiv.org/pdf/1804.04272.pdf)
* https://www.researchgate.net/scientific-contributions/2107227289_Eldad_Haber


Residual networks as discretizations of dynamic systems:
$$
Y_1 = Y_0 +h \sigma(K_0 Y_0 + b_0)\\
\vdots  \\
Y_N = Y_{N-1} +h \sigma(K_{N-1} Y_{N-1} + b_{N-1})
$$

This is nothing but a forward Euler discretization of the `Ordinary Differential Equation (ODE)`:
$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

The goal is to plan a path (via $K$ and $b$) such that the initial data can be linearly separated.

<img src="http://www.mathcs.emory.edu/~lruthot/img/DeepLearning.png" width="80%" />

Another idea is to ensure stability by design / constraints on $\sigma$ and $K(t), b(t)$.

ResNet with antisymmetric transformation matrix:
$$\partial Y(t)=\sigma([K(t)-K(t)^T] Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

Hamiltonian-like ResNet
$$\frac{\mathrm d}{\mathrm d t}(Y(t), Z(t))^T=\sigma[(K(t)Z(t), -K(t)^T Y(t))^T + b(t)], t\in[0, T].$$

`Parabolic Residual Neural Networks`

$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

`Hyperbolic Residual Neural Networks`

$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

`Hamiltonian CNN`

$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$



- [Numerical methods for deep learning](https://github.com/IPAIopen/NumDL-CourseNotes)
- [Short Course on Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)
- [Deep Neural Networks Motivated By Ordinary Differential Equations](http://www.mathcs.emory.edu/~lruthot/talks/2019-LR-IPAM-ODE-handout.pdf)
- [Continuous Models: Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/3-NumDNNshort-ContinuousModels.pdf)
- [Fully Hyperbolic Convolutional Neural Networks](https://arxiv.org/abs/1905.10484)
- https://eldad-haber.webnode.com/selected-talks/
- http://www.mathcs.emory.edu/~lruthot/courses/NumDL/3-NumDNNshort-ContinuousModels.pdf

<img src="https://pic4.zhimg.com/80/v2-542db02f15d327ccc7558df7a8e6e137_hd.jpg" width="60%"/>

`Numerical differential equation inspired networks`:
$$Y_{t+1} = (1-k_t)Y_{t-1} + k_t Y_t + h \sigma(K_{t} Y_{t} + b_{t})\tag{Linear multi-step structure}.$$


- [Bridging Deep Architects and Numerical Differential Equations](https://web.stanford.edu/~yplu/proj/lm/)
- [BRIDGING DEEP NEURAL NETWORKS AND DIFFERENTIAL EQUATIONS FOR IMAGE ANALYSIS AND BEYOND](http://helper.ipam.ucla.edu/publications/glws3/glws3_15460.pdf)
- [Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations](https://arxiv.org/abs/1710.10121)
- http://bicmr.pku.edu.cn/~dongbin/
- https://arxiv.org/pdf/1906.02762.pdf  
- [Neural ODE Paper List](https://zhuanlan.zhihu.com/p/87999707)


* [A Multiscale and Multidepth Convolutional Neural Network for Remote Sensing Imagery Pan-Sharpening](https://ieeexplore.ieee.org/document/8281501)
* https://arxiv.org/abs/1808.02376
* [Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images](https://www.nature.com/articles/s41598-018-22871-z)

`MgNet`

[As the solution space is often the dual of the data space in PDEs, the
analogous concept of feature space and data space (which are dual to each other) is introduced
in CNN. With such connections and new concept in the unified model, the function of various
convolution operations and pooling used in CNN can be better understood.](https://arxiv.org/pdf/1901.10415.pdf)

- [MgNet: A Unified Framework of Multigrid and Convolutional Neural Network](https://arxiv.org/pdf/1901.10415.pdf)
- http://www.multigrid.org/img2019/img2019/Index/shortcourse.html
- https://deepai.org/machine-learning/researcher/jinchao-xu


_____


- [MA 721: Topics in Numerical Analysis: Deep Learning](http://www.ms.uky.edu/~qye/MA721/ma721F17.html)
- http://www.mathcs.emory.edu/~lruthot/teaching.html
- https://www.math.ucla.edu/applied/cam
- http://www.mathcs.emory.edu/~lruthot/
- [Automatic Differentiation of Parallelised Convolutional Neural Networks - Lessons from Adjoint PDE Solvers](https://autodiff-workshop.github.io/slides/Hueckelheim_nips_autodiff_CNN_PDE.pdf)
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs.](https://www.math.tu-berlin.de/fachgebiete_ag_modnumdiff/angewandtefunktionalanalysis/v_menue/mitarbeiter/kutyniok/v_menue/kutyniok_publications/)
- https://raoyongming.github.io/  
- https://sites.google.com/prod/view/haizhaoyang/
- https://github.com/HaizhaoYang
- https://www.stat.uchicago.edu/events/rtg/index.shtml


### Control Theory and Deep Learning

[It arose out of control theory literature when people were trying to identify highly complex and nonlinear dynamical systems. Neural networks – artificial neural networks – were first used in a supervised learning scenario in control theory. Hornik, if I remember correctly, was the first to find that neural networks were universal approximators.](http://scriptedonachip.com/ml-control)

> Supervised Deep Learning Problem
Given training data, $Y_0$, and labels, $C$, find network parameters $\theta$ and
classification weights $W, \mu$ such that the DNN predicts the data-label
relationship (and generalizes to new data), i.e., solve
$$\operatorname{minimize}_{ \theta,W,\mu} loss[g(W, \mu), C] + regularizer[\theta,W,\mu]$$

This can rewrite in a compact form
$$\operatorname{minimize}_{ \theta,W,\mu} loss[g(W(T)Y(T)+\mu), C] + regularizer[\theta,W,\mu]\\
\text{subject to  }\partial_t Y(t) = f (Y(t), \theta(t)), Y(0) = Y_0.$$

- [Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective](https://arxiv.org/abs/1908.10920)
- [An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks](http://proceedings.mlr.press/v80/li18b/li18b.pdf)
- [Dynamic System and Optimal Control Perspective of Deep Learning](https://web.stanford.edu/~yplu/DynamicOCNN.pdf)
- [A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1751636)
- [Deep learning as optimal control problems: models and numerical methods](https://arxiv.org/pdf/1904.05657.pdf)
- [A Mean-Field Optimal Control Formulation of Deep Learning](https://deepai.org/publication/a-mean-field-optimal-control-formulation-of-deep-learning)
- [Control Theory and Machine Learning](http://scriptedonachip.com/ml-control)
- [Advancing Systems and Control Research in the Era of ML and AI](https://faculty.sites.uci.edu/khargonekar/files/2018/04/Control_ML_AI_Final.pdf)
- http://marcogallieri.micso.it/Home.html
- [Deep Learning meets Control Theory: Research at NNAISENSE and Polimi](http://www.eventideib.polimi.it/events/deep-learning-meets-control-theory-research-at-nnaisense-and-polimi/)
- [Machine Learning-based Control](https://github.com/lakehanne/awesome-neurocontrol)
- [CAREER: A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1751636)
- https://www.zhihu.com/question/315809187/answer/623687046
- https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR19-FOCNet.pdf


### Neural Ordinary Differential Equations

`Neural ODE`

- [Neural Ordinary Differential Equations](http://www.cs.toronto.edu/~rtqichen/pdfs/neural_ode_slides.pdf)

<img src="https://rkevingibson.github.io/img/ode_networks_1.png" width="80%" />

+ [NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data](https://www.arxiv-vanity.com/papers/1908.03190/)
+ [Neural Ordinary Differential Equations and Adversarial Attacks](https://rajatvd.github.io/Neural-ODE-Adversarial/)
+ [Neural Dynamics and Computation Lab](http://ganguli-gang.stanford.edu/)
+ [NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data](https://arxiv.org/abs/1908.03190)
+ https://math.ethz.ch/sam/research/reports.html?year=2019


## Dynamics and Deep Learning

- http://roseyu.com/
- [A Proposal on Machine Learning via Dynamical Systems](https://link.springer.com/article/10.1007/s40304-017-0103-z)
- http://www.scholarpedia.org/article/Attractor_network
- [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf)
- [An Attractor-Based Complexity Measurement for Boolean Recurrent Neural Networks](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3984152/)
- [Deep learning for universal linear embeddings of nonlinear dynamics](https://doaj.org/article/9d9172e9bf324cc6ac6d48ff8e234a85)
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](http://ganguli-gang.stanford.edu/pdf/DynamLearn.pdf)
- [Continuous attractors of higher-order recurrent neural networks with infinite neurons](https://www.sciencedirect.com/science/article/pii/S0925231213009338)
- https://www.researchgate.net/profile/Jiali_Yu3
- [Markov Transitions between Attractor States in a Recurrent Neural Network](https://cbmm.mit.edu/sites/default/files/publications/aaai-abstract%20%281%29.pdf)
- [A Survey on Machine Learning Applied to Dynamic Physical Systems](https://sagarverma.github.io/others/lit_rev_physics.pdf)
- https://deepdrive.berkeley.edu/project/dynamical-view-machine-learning-systems



### Stability For Neural Networks

- https://folk.uio.no/vegarant/
- https://www.mn.uio.no/math/english/people/aca/vegarant/index.html
- https://arxiv.org/pdf/1710.11029.pdf
- http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Raja.pdf
- https://arxiv.org/abs/1705.03341
- https://izmailovpavel.github.io/
- https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zheng_Improving_the_Robustness_CVPR_2016_paper.pdf

## Differential Equation and Deep Learning

This section is on how to use deep learning or more general machine learning to solve  differential equation numerically.

We derive upper bounds on the complexity of ReLU neural networks approximating the solution maps of parametric partial differential equations.
In particular, without any knowledge of its concrete shape, we use the inherent low-dimensionality of the solution manifold to obtain approximation rates
which are significantly superior to those provided by classical approximation results. 
We use this low dimensionality to guarantee the existence of a reduced basis. 
[Then, for a large variety of parametric partial differential equations, we construct neural networks that yield approximations of the parametric maps not suffering from a curse of dimension and essentially only depending on the size of the reduced basis.](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)

- https://math.ethz.ch/sam/research/reports.html?year=2019
- https://aimath.org/workshops/upcoming/deeppde/
- https://github.com/IBM/pde-deep-learning
- https://arxiv.org/abs/1804.04272
- https://deepai.org/machine-learning/researcher/weinan-e
- https://deepxde.readthedocs.io/en/latest/
- https://github.com/IBM/pde-deep-learning
- https://github.com/ZichaoLong/PDE-Net
- https://github.com/amkatrutsa/DeepPDE
- https://github.com/maziarraissi/DeepHPMs
- https://github.com/markovmodel/deeptime
- [Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations](https://arxiv.org/abs/1801.06637)
- [SPNets: Differentiable Fluid Dynamics for Deep Neural Networks](https://rse-lab.cs.washington.edu/papers/spnets2018.pdf)
- https://maziarraissi.github.io/DeepHPMs/
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)
- [Deep Approximation via Deep Learning](http://ins.sjtu.edu.cn:3300/conferences/7/talks/314)

### Deep Learning for PDEs

- [The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems](https://link.springer.com/article/10.1007/s40304-018-0127-z)
- [Solving Nonlinear and High-Dimensional Partial Differential Equations via Deep Learning](http://utstat.toronto.edu/~ali/papers/PDEandDeepLearning.pdf)
- [DGM: A deep learning algorithm for solving partial differential equations](https://www.sciencedirect.com/science/article/pii/S0021999118305527)
- [NeuralNetDiffEq.jl: A Neural Network solver for ODEs](https://julialang.org/blog/2017/10/gsoc-NeuralNetDiffEq)
- [PIMS CRG Summer School: Deep Learning for Computational Mathematics](https://www.pims.math.ca/scientific-event/190722-pcssdlcm)

* https://arxiv.org/abs/1806.07366
* https://mat.univie.ac.at/~grohs/
* https://rse-lab.cs.washington.edu/
* http://www.ajentzen.de/
* https://web.math.princeton.edu/~jiequnh/

### $\mathcal H$ matrix and deep learning

[In this work we introduce a new multiscale artificial neural network based on the structure of H-matrices. This network generalizes the latter to the nonlinear case by introducing a local deep neural network at each spatial scale. Numerical results indicate that the network is able to efficiently approximate discrete nonlinear maps obtained from discretized nonlinear partial differential equations, such as those arising from nonlinear Schodinger equations and the KohnSham density functional theory.](https://web.stanford.edu/~lexing/mnnh.pdf)

* [A multiscale neural network based on hierarchical matrices](https://web.stanford.edu/~lexing/mnnh.pdf)
* [A multiscale neural network based on hierarchical nested bases](https://link.springer.com/article/10.1007%2Fs40687-019-0183-3)

[We aim to build a theoretical foundation for the analysis of deep neural networks to answer questions such as "What are the correct approximation spaces for deep neural networks?", "What is the advantage of deep versus shallow networks?", or "To which extent are deep neural networks able to detect low dimensional structures in high dimensional data?".](https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks)
- https://www.researchgate.net/profile/Gitta_Kutyniok
- https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks
- https://www.academia-net.org/profil/prof-dr-gitta-kutyniok/1133890
- https://www.tu-berlin.de/index.php?id=168945
- https://www.math.tu-berlin.de/?108957
- [Deep Learning: An Introduction for Applied Mathematicians](https://arxiv.org/abs/1801.05894)

###  Stochastic Differential Equations and Deep Learning

- [Neural Jump SDEs (Jump Diffusions) and Neural PDEs](http://www.stochasticlifestyle.com/neural-jump-sdes-jump-diffusions-and-neural-pdes/)
- [Deep-Learning Based Numerical BSDE Method for Barrier Options](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3366314)
- [Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations](https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2017/2017-49.pdf)

### Finite Element Methods and Deep Learning

- http://www.multigrid.org/index.php?id=13
- http://casopisi.junis.ni.ac.rs/index.php/FUMechEng/article/view/309
- http://people.math.sc.edu/imi/DASIV/
- [Deep ReLU Networks and High-Order Finite Element Methods](https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2019/2019-07.pdf)
- https://math.psu.edu/events/35992
- [Neural network for constitutive modelling in finite element analysis](https://olemiss.edu/sciencenet/trefftz/Trefftz/Exeter/Javadi.pdf)
- https://arxiv.org/abs/1807.03973
- [A deep learning approach to estimate stress distribution: a fast and accurate surrogate of finite-element analysis](https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0844)
- [An Integrated Machine Learning and Finite Element Analysis Framework, Applied to Composite Substructures including Damage](https://repository.tudelft.nl/islandora/object/uuid%3A615f2151-bcae-4e78-a2cb-3f1891a28275)
- https://github.com/oleksiyskononenko/mlfem
- https://people.math.gatech.edu/~wliao60/
- https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/main.pdf

## Approximation Theory for Deep Learning

Universal approximation theory show the expression power of deep neural network of some wide while shallow neural network.
The section will extend the approximation to the deep neural network.

[We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in $L^2(\mathbb R^d)$. In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy.](https://epubs.siam.org/doi/pdf/10.1137/18M118709X)

- [Deep Neural Network Approximation Theory](https://arxiv.org/abs/1901.02220)
- [Approximation Analysis of Convolutional Neural Networks](https://cpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/d/11132/files/2019/07/paper_cnn_copy.pdf)
- [Deep vs. shallow networks : An approximation theory perspective](https://arxiv.org/abs/1608.03287)
- [Deep Neural Network Approximation Theory](https://arxiv.org/abs/1901.02220)
- [Provable approximation properties for deep neural networks](https://cpsc.yale.edu/sites/default/files/files/tr1513(1).pdf)
- [Optimal Approximation with Sparsely Connected Deep Neural Networks](https://epubs.siam.org/doi/pdf/10.1137/18M118709X)
- [Deep Learning: Approximation of Functions by Composition](http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf)
- [Deep Neural Networks: Approximation Theory and Compositionality](http://www.mit.edu/~9.520/fall16/Classes/deep_approx.html)
- [DNN Bonn](http://voigtlaender.xyz/DNNBonnHandout.pdf)
- [From approximation theory to machine learning](http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf)
- [Collapse of Deep and Narrow Neural Nets](https://arxiv.org/abs/1808.04947)
- [Nonlinear Approximation and (Deep) ReLU Networks](https://www.math.tamu.edu/~foucart/publi/DDFHP.pdf)
- [Deep Approximation via Deep Learning](http://www.ipam.ucla.edu/abstract/?tid=15953&pcode=GLWS3)
- [Convolutional Neural Networks for Steady Flow Approximation](https://github.com/loliverhennigh/Steady-State-Flow-With-Neural-Nets)
- https://www.eurandom.tue.nl/wp-content/uploads/2018/11/Johannes-Schmidt-Hieber-lecture-1-2.pdf
- https://arxiv.org/abs/2006.00294
- [Efficient approximation of high-dimensional functions with deep neural networks](https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2019/2019-64_fp.pdf)


#### Workshop

- https://www.mfo.de/occasion/1842b
- https://www.mfo.de/occasion/1947a
- https://github.com/juliusberner/oberwolfach_workshop
- [DGD Approximation Theory Workshop](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Petersen/DGD_Approximation_Theory.pdf)

#### Labs and Groups 

- https://deepai.org/profile/julius-berner
- https://www.cityu.edu.hk/ma/people/profile/zhoudx.htm
- https://dblp.uni-trier.de/pers/hd/y/Yang:Haizhao
- https://math.duke.edu/people/ingrid-daubechies
- http://www.pc-petersen.eu/
- https://wwwhome.ewi.utwente.nl/~schmidtaj/
- https://personal-homepages.mis.mpg.de/montufar/
- https://www.math.tamu.edu/~foucart/
- http://www.damtp.cam.ac.uk/user/sl767/#about
- http://voigtlaender.xyz/publications.html

### The F-Principle

> Understanding the training process of Deep Neural Networks (DNNs) is a fundamental problem in the area of deep learning. The study of the training process from the frequency perspective makes important progress in understanding the strength and weakness of DNN, such as generalization and converging speed etc., which may consist in “a reasonably complete picture about the main reasons behind the success of modern machine learning” (E et al., 2019).

> The “Frequency Principle” was first named in the paper (Xu et al., 2018), then (Xu 2018; Xu et al., 2019) use more convincing experiments and a simple theory to demonstrate the university of the Frequency Principle. Bengio's paper (Rahaman et al., 2019) also uses the the simple theory in (Xu 2018; Xu et al., 2019) to understand the mechanism underlying the Frequency Principle for ReLU activation function. Note that the second version of Rahaman et al., (2019) points out this citation clearly but they reorganize this citation to “related works” in the final version. Later, Luo et al., (2019) studies the Frequency Principle in the general setting of deep neural networks and mathematically proves Frequency Principle with the assumption of infinite samples. Zhang et al., (2019) study the Frequency Principle in the NTK regime with finite sample points. Zhang et al., (2019) explicitly shows that the converging speed for each frequency and can accurately predict the learning results.

[We aim to develop a theoretical framework on Fourier domain to analyze the Deep Neural Network (DNN) training process and understand the DNN generalization. We exemplified our theoretical results through DNNs fitting 1-d functions and the MNIST dataset.](https://www.researchgate.net/project/Deep-learning-in-Fourier-domain)

- [Deep learning in Fourier domain](https://www.researchgate.net/project/Deep-learning-in-Fourier-domain)
- [Deep Learning Theory: The F-Principle and An Optimization Framework](http://ins.sjtu.edu.cn:3300/conferences/7/talks/319)
- [Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks](https://arxiv.org/abs/1901.06523)
- [Nonlinear Collaborative Scheme for Deep Neural Networks](https://arxiv.org/abs/1811.01316)
- [The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies](https://arxiv.org/abs/1906.00425)
- [Frequency Principle in Deep Learning with General Loss Functions and Its Potential Application](https://arxiv.org/abs/1811.10146)
- [Theory of the Frequency Principle for General Deep Neural Networks](https://arxiv.org/pdf/1906.09235v1.pdf)
- [Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks](https://arxiv.org/pdf/1905.10264.pdf)
- https://www.researchgate.net/profile/Zhiqin_Xu
- https://github.com/xuzhiqin1990/F-Principle
- https://ins.sjtu.edu.cn/people/xuzhiqin/

## Inverse Problem and Deep Learning

[There is a long history of algorithmic development for solving inverse problems arising in sensing and imaging systems and beyond. 
Examples include medical and computational imaging, compressive sensing, as well as community detection in networks. Until recently, 
most algorithms for solving inverse problems in the imaging and network sciences were based on static signal models derived from physics or intuition, 
such as wavelets or sparse representations.](https://deep-inverse.org/)

[Today](https://deep-inverse.org/), the best performing approaches for the aforementioned image reconstruction and sensing problems are based on deep learning, 
which learn various elements of the method including 
i) signal representations, 
ii) stepsizes and parameters of iterative algorithms, 
iii) regularizers, and iv) entire inverse functions. 
For example, it has recently been shown that solving a variety of inverse problems by transforming an iterative, physics-based algorithm into a deep network 
whose parameters can be learned from training data, offers faster convergence and/or a better quality solution. 
Moreover, even with very little or no learning, deep neural networks enable superior performance for classical linear inverse problems 
such as denoising and compressive sensing. Motivated by those success stories, researchers are redesigning traditional imaging and sensing systems.

* [MATH + X SYMPOSIUM ON INVERSE PROBLEMS AND DEEP LEARNING IN SPACE EXPLORATION](https://earthscience.rice.edu/mathx2019/)
- [Sixteenth International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research](http://cpaior2019.uowm.gr/)
- https://github.com/mughanibu/Deep-Learning-for-Inverse-Problems
- [Accurate Image Super-Resolution Using Very Deep Convolutional Networks](https://cv.snu.ac.kr/research/VDSR/)
- https://earthscience.rice.edu/mathx2019/
- https://www.researchgate.net/publication/329395098_On_Deep_Learning_for_Inverse_Problems
- [Deep Learning and Inverse Problem](https://www.dlip.org/)
- https://www.scec.org/publication/8768
- https://amds123.github.io/
- https://github.com/IPAIopen
- https://imaginary.org/snapshot/deep-learning-and-inverse-problems
- https://www.researchgate.net/scientific-contributions/2150388821_Jaweria_Amjad
- https://zif.ai/inverse-reinforcement-learning/
- [Physics Based Machine Learning for Inverse Problems](https://kailaix.github.io/ADCMESlides/Inverse.pdf)
- https://www.ece.nus.edu.sg/stfpage/elechenx/Papers/TGRS_Learning.pdf

### Deep Learning for Inverse Problems

- [Deep Learning for Inverse Problems](https://arxiv.org/abs/1803.00092)
- [Solving inverse problems with deep networks](https://deep-inverse.org/)
- [Neumann Networks for Inverse Problems in Imaging](https://arxiv.org/abs/1901.03707)
- https://deepai.org/publication/unsupervised-deep-learning-algorithm-for-pde-based-forward-and-inverse-problems

### Deep Inverse Optimization

- [deep inverse optimization](https://github.com/tankconcordia/deep_inv_opt)
- https://ori.ox.ac.uk/deep-irl/
- https://physai.sciencesconf.org/data/pages/perez_2019_03_Institut_Pascal_AI_and_Physics_noanim.pdf
- https://neurips.cc/virtual/2020/public/workshop_16128.html
- https://data102.org/fa20/

## Random Matrix Theory and Deep Learning

Random matrix focus on the matrix, whose entities are sampled from  some specific probability distribution.
Weight matrices in deep nerual network are initialed in random.
However, the model is over-parametered and it is hard to verify the role of one individual parameter.

- http://romaincouillet.hebfree.org/
- https://zhenyu-liao.github.io/
- https://dionisos.wp.imt.fr/
- https://project.inria.fr/paiss/
- https://zhenyu-liao.github.io/activities/
- [Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning](https://arxiv.org/abs/1810.01075)
- [Recent Advances in Random Matrix Theory for Modern Machine Learning](https://zhenyu-liao.github.io/pdf/pre/Matrix_talk_liao_handout.pdf)
- [Features extraction using random matrix theory](https://ir.library.louisville.edu/cgi/viewcontent.cgi?article=2227&context=etd)
- [Nonlinear random matrix theory for deep learning](https://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [A RANDOM MATRIX APPROACH TO NEURAL NETWORKS](https://arxiv.org/pdf/1702.05419.pdf)
- [A Random Matrix Approach to Echo-State Neural Networks](http://proceedings.mlr.press/v48/couillet16.pdf)
- [Harnessing neural networks: A random matrix approach](https://hal.archives-ouvertes.fr/hal-01962073)
- [Tensor Programs: A Swiss-Army Knife for Nonlinear Random Matrix Theory of Deep Learning and Beyond](https://www.csail.mit.edu/event/tensor-programs-swiss-army-knife-nonlinear-random-matrix-theory-deep-learning-and-beyond)
- [Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation](https://arxiv.org/abs/1902.04760)
- [Random Matrix Theory and its Innovative Applications∗](http://www-math.mit.edu/~edelman/publications/random_matrix_theory_innovative.pdf)
- https://romaincouillet.hebfree.org/docs/conf/ELM_icassp.pdf
- https://romaincouillet.hebfree.org/docs/conf/NN_ICML.pdf
- http://www.vision.jhu.edu/tutorials/CVPR16-Tutorial-Math-Deep-Learning-Raja.pdf
- [A Random Matrix Framework for BigData Machine Learning](https://www.lri.fr/TAU_seminars/videos/Romain_Couillet_12juin2017/talk_lri.pdf)

### Nonlinear Random Matrix Theory

- https://ai.google/research/pubs/pub46342
- http://people.cs.uchicago.edu/~pworah/nonlinear_rmt.pdf
- [A SWISS-ARMY KNIFE FOR NONLINEAR RANDOM MATRIX THEORY OF DEEP LEARNING AND BEYOND](https://toc.csail.mit.edu/node/1314)
- https://simons.berkeley.edu/talks/9-24-mahoney-deep-learning
- https://cs.stanford.edu/people/mmahoney/
- https://www.stat.berkeley.edu/~mmahoney/f13-stat260-cs294/
- https://arxiv.org/abs/1902.04760
- https://melaseddik.github.io/
- https://thayafluss.github.io/

## Deep learning and Optimal Transport

Optimal transport (OT) provides a powerful and flexible way to compare probability measures, 
of all shapes: absolutely continuous, degenerate, or discrete. 
This includes of course point clouds, histograms of features, and more generally datasets, parametric densities or generative models. 
Originally proposed by Monge in the eighteenth century, 
[this theory later led to Nobel Prizes for Koopmans and Kantorovich as well as Villani’s Fields Medal in 2010.](http://otml17.marcocuturi.net/)

- [Optimal Transport & Machine Learning](http://otml17.marcocuturi.net/)
- [Topics on Optimal Transport in Machine Learning and Shape Analysis (OT.ML.SA)](https://people.math.osu.edu/memolitechera.1/courses/cse-topics-2018/)
- https://www-obelix.irisa.fr/files/2017/01/postdoc-Obelix.pdf
- http://www.cis.jhu.edu/~rvidal/talks/learning/StructuredFactorizations.pdf
- https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/
- https://www.louisbachelier.org/wp-content/uploads/2017/07/170620-ilb-presentation-gabriel-peyre.pdf
- http://people.csail.mit.edu/davidam/
- https://www.birs.ca/events/2020/5-day-workshops/20w5126
- https://github.com/hindupuravinash/nips2017
- [Selection dynamics for deep neural networks](https://arxiv.org/abs/1905.09076v1)
- https://people.math.osu.edu/memolitechera.1/index.html


### Generative Models and Optimal Transport

- https://www.researchgate.net/publication/317378242_GAN_and_VAE_from_an_Optimal_Transport_Point_of_View
- https://arxiv.org/abs/1710.05488
- http://www.dataguru.cn/article-14562-1.html
- http://cmsa.fas.harvard.edu/wp-content/uploads/2018/06/David_Gu_Harvard.pdf
- http://www.dataguru.cn/article-14563-1.html
- http://games-cn.org/games-webinar-20190509-93/
- https://www3.cs.stonybrook.edu/~gu/

## Geometric Analysis Approach to AI

Why and how that deep learning works well on different tasks remains a mystery from a theoretical perspective. In this paper we draw a geometric picture of the deep learning system by finding its analogies with two existing geometric structures, the geometry of quantum computations and the geometry of the diffeomorphic template matching. 
In this framework, we give the geometric structures of different deep learning systems including convolutional neural networks, residual networks, recursive neural networks, recurrent neural networks and the equilibrium prapagation framework. 
[We can also analysis the relationship between the geometrical structures and their performance of different networks in an algorithmic level so that the geometric framework may guide the design of the structures and algorithms of deep learning systems.](https://arxiv.org/pdf/1710.10784.pdf)


- [Machine Learning on Geometrical Data CSE291-C00 - Winter 2019](https://cse291-i.github.io/)
- [ABC Dataset A Big CAD Model Dataset For Geometric Deep Learning](https://deep-geometry.github.io/abc-dataset/)
- [Into the Wild: Machine Learning In Non-Euclidean Spaces](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)
- [How deep learning works — The geometry of deep learning](https://arxiv.org/pdf/1710.10784.pdf)
- http://cmsa.fas.harvard.edu/geometric-analysis-ai/
- http://inspirehep.net/record/1697651
- https://diglib.eg.org/handle/10.2312/2631996
- http://ubee.enseeiht.fr/skelneton/
- https://biomedicalimaging.org/2019/tutorials/
- [Geometric View to Deep Learning](http://valser.org/article-269-1.html)
- [GEOMETRIC IDEAS IN MACHINE LEARNING: FROM DEEP LEARNING TO INCREMENTAL OPTIMIZATION](https://www.isi.edu/events/calendar/12459/)
- [Deep Learning Theory: Geometric Analysis of Capacity, Optimization, and Generalization for Improving Learning in Deep Neural Networks](https://cordis.europa.eu/project/rcn/214602/factsheet/en)
- [Workshop IV: Deep Geometric Learning of Big Data and Applications](http://www.ipam.ucla.edu/programs/workshops/workshop-iv-deep-geometric-learning-of-big-data-and-applications/)
- [Robustness and geometry of deep neural networks](https://gateway.newton.ac.uk/sites/default/files/asset/doc/1905/Alhussein_Fawzi.pdf)
- [A geometric view of optimal transportation and generative model](https://www.sciencedirect.com/science/article/pii/S0167839618301249)
- [Optimal Transport Theory the New Math for Deep Learning](https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/)
- [GeoNet: Deep Geodesic Networks for Point Cloud Analysis](http://openaccess.thecvf.com/content_CVPR_2019/papers/He_GeoNet_Deep_Geodesic_Networks_for_Point_Cloud_Analysis_CVPR_2019_paper.pdf)
- http://www.stat.uchicago.edu/~lekheng/
- https://www.nsf.gov/awardsearch/showAward?AWD_ID=1418255
- https://nsf-tripods.org/institutes/
- https://users.math.msu.edu/users/wei/
- https://www.darpa.mil/program/hierarchical-identify-verify-exploit
- [The Loss Surface Of Deep Linear Networks Viewed
Through The Algebraic Geometry Lens](http://www.tianranchen.org/research/papers/deep-linear.pdf)

### Tropical Geometry of Deep Neural Networks

> The basic idea of tropical geometry is to study the same kinds of questions as in standard algebraic geometry, but change what we mean when we talk about ‘polynomial equations’. 

- [Tropical Geometry of Deep Neural Networks](https://arxiv.org/pdf/1805.07091.pdf)
- https://opendatagroup.github.io/data%20science/2019/04/11/tropical-geometry.html
- https://www.stat.uchicago.edu/~lekheng/
- https://mathsites.unibe.ch/siamag19/
- https://www.math.ubc.ca/~erobeva/seminar.html
- https://sites.google.com/view/maag2019/home
- https://sites.google.com/site/feliper84/
- https://deepai.org/publication/a-tropical-approach-to-neural-networks-with-piecewise-linear-activations
- [ReLu and Maxout Networks and Their Possible Connections to Tropical Methods](https://www.symbiont-project.org/events/Slides-2018-03/SYMBIONT-2018-03-zimmermann.pdf)
- [Applications of Tropical Geometry in Deep Neural Networks](https://repository.kaust.edu.sa/bitstream/handle/10754/662473/Masters_Thesis%20(6).pdf?sequence=10)

## Topology and Deep Learning

[We perform `topological data analysis`](https://arxiv.org/abs/1811.01122) 
on the internal states of convolutional deep neural networks to develop an understanding of the computations 
that they perform. We apply this understanding to modify the computations so as to (a) speed up computations and (b) improve generalization 
from one data set of digits to another. 
One byproduct of the analysis is the production of a geometry on new sets of features on data sets of images, 
and use this observation to develop a methodology for constructing analogues of CNN's for many other geometries, 
including the graph structures constructed by topological data analysis.

- [Topological Methods for Machine Learning](http://topology.cs.wisc.edu/)
- [A Topology Layer for Machine Learning](http://ai.stanford.edu/blog/topologylayer/)
- [Topological Approaches to Deep Learning](https://arxiv.org/abs/1811.01122)
- https://www.gaotingran.com/
- [Topology based deep learning for biomolecular data](https://users.math.msu.edu/users/wei/AIM.pdf)
- [RESEARCH ARTICLE TopologyNet: Topology based deep convolutional and multi-task neural networks for biomolecular property predictions](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005690)
- [Exposition and Interpretation of the Topology of Neural Networks](https://arxiv.org/abs/1810.03234)
- https://zhuanlan.zhihu.com/p/26515275
- [Applying Topological Persistence in Convolutional Neural Network for Music Audio Signals](https://arxiv.org/pdf/1608.07373.pdf)
- [Towards a topological–geometrical theory of group equivariant non-expansive operators for data analysis and machine learning](https://www.nature.com/articles/s42256-019-0087-3)

### Topology Optimization and  Deep Learning

- http://www.inescporto.pt/~jsc/publications/conferences/2019RAraujoMICCAI.pdf
- https://yangliang.github.io/pdf/ijcai19_to.pdf
- https://www.dbs.ifi.lmu.de/~tresp/
- [A Novel Topology Optimization Approach using Conditional Deep Learning](https://arxiv.org/abs/1901.04859)
- https://arxiv.org/ftp/arxiv/papers/1901/1901.07761.pdf
- https://www.researchgate.net/publication/322568237_Deep_learning_for_determining_a_near-optimal_topological_design_without_any_iteration
- [Topology Optimization Accelerated by Deep Learning](https://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/74695/1/MAGCON-18-11-1747-final.pdf)

## Algebra and Deep Learning

Except the matrix and tensor decomposotion for accelerating the deep neural network, `Tensor network` is close to deep learning model.

- http://people.cs.uchicago.edu/~risi/
- https://ttic.uchicago.edu/~shubhendu/

### Tensor network

- [Linear Algebra and Learning from Data](http://math.mit.edu/~gs/learningfromdata/)
- [Accelerating deep neural networks with tensor decompositions](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning)
- [An Algebraic Perspective on Deep Learning](http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10605.pdf)
- [Tensor Networks in a Nutshell](https://arxiv.org/abs/1708.00006)
- [A library for easy and efficient manipulation of tensor networks.](https://github.com/google/tensornetwork)
- http://tensornetworktheory.org/
- https://www.perimeterinstitute.ca/research/research-initiatives/tensor-networks-initiative
- https://github.com/emstoudenmire/TNML
- http://itensor.org/
- http://users.cecs.anu.edu.au/~koniusz/
- https://deep-learning-tensorflow.readthedocs.io/en/latest/

### Group Equivariant Convolutional Networks

- https://github.com/tscohen/gconv_experiments
- http://dalimeeting.org/dali2019b/workshop-05-02.html
- https://erikbekkers.bitbucket.io/
- https://staff.fnwi.uva.nl/m.welling/
- https://www.ics.uci.edu/~welling/
- http://ibis.t.u-tokyo.ac.jp/suzuki/
- http://www.mit.edu/~kawaguch/
- https://www.4tu.nl/ami/en/Agenda-Events/

### Complex Valued Neural Networks

Aizenberg, Ivaskiv, Pospelov and Hudiakov (1971) (former Soviet Union) proposed a complex-valued neuron model for the first time, and although it was only available in Russian literature, their work can now be read in English (Aizenberg, Aizenberg & Vandewalle, 2000). Prior to that time, most researchers other than Russians had assumed that the first persons to propose a complex-valued neuron were Widrow, McCool and Ball (1975). Interest in the field of neural networks started to grow around 1990, and various types of complex-valued neural network models were subsequently proposed. Since then, their characteristics have been researched, making it possible to solve some problems which could not be solved with the real-valued neuron, and to solve many complicated problems more simply and efficiently.

- http://what-when-how.com/artificial-intelligence/complex-valued-neural-networks-artificial-intelligence/

> The complex-valued Neural Network is an extension of a (usual) real-valued neural network, whose input and output signals and parameters such as weights and thresholds are all complex numbers (the activation function is inevitably a complex-valued function).

- https://staff.aist.go.jp/tohru-nitta/HNN.html
- https://staff.aist.go.jp/tohru-nitta/CNN.html
- https://github.com/ChihebTrabelsi/deep_complex_networks
- https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html
- https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2011-42.pdf
- https://www.microsoft.com/en-us/research/uploads/prod/2018/04/Deep-Complex-Networks.pdf
- https://core.ac.uk/reader/41356536



### Quaternion Neural Networks

[It looks like Deep (Convolutional) Neural Networks are really powerful. ](https://www.simonwenkel.com/2019/07/15/Capsule-Networks-and-other-neural-architectures.html)
However, there are situations where they don’t deliver as expected. 
I assume that perhaps many are happy with pre-trained VGG, Resnet, YOLO, SqueezeNext, MobileNet, etc. models because they are “good enough”, 
even though they break quite easily on really realistic problems and require tons of training data. 
IMHO there are much smarter approaches out there, 
which are neglected/ignored. I don’t want to argue why they are ignored but I want to provide a list with other useful architectures.

Instead of staying with real numbers, we should have a look at complex numbers as well. 
Let’s remember the single reason why we use complex numbers ($C$) or quaternions ($\mathcal H$). 
The most important reason why we use complex numbers is not to solve $x^2=−1$.
The reason why we use complex numbers for everything that involves waves etc. is that we are lazy or efficient ;). 
Who wants to waste time writing down and solving a bunch of trignometric identities. 
The same is true for quaternions in robotics. Speaking in terms of computer science, 
we are using a much more efficient data structure/representation. 
It seems like complex valued neural networks as well as quaternion, 
which are a different kind of complex numbers for the mathematical correct reader of this post, 
seem to outperform real valued neural networks while using less parameters. This makes sense because we are using a different data structure 
[that itself helps to represent certain things in a much more useful way.](https://www.simonwenkel.com/2019/07/15/Capsule-Networks-and-other-neural-architectures.html)

- https://arxiv.org/abs/1903.08478
- [Introduction to Quaternion Neural Networks](https://www.simonwenkel.com/projects/introduction-to-quaternion-neural-networks.html)
- [Capsule Networks and other neural architectures that are less known](https://www.simonwenkel.com/2019/07/15/Capsule-Networks-and-other-neural-architectures.html)
- https://github.com/Orkis-Research/Quaternion-Convolutional-Neural-Networks-for-End-to-End-Automatic-Speech-Recognition

## Probabilistic Theory and Deep Learning


[Probabilistic Deep Learning with Python teaches the increasingly popular probabilistic approach to deep learning that allows you to tune and refine your results more quickly and accurately without as much trial-and-error testing. Emphasizing practical techniques that use the Python-based Tensorflow Probability Framework, you’ll learn to build highly-performant deep learning applications that can reliably handle the noise and uncertainty of real-world data.](https://www.manning.com/books/probabilistic-deep-learning-with-python)

- [Probabilistic Framework for Deep Learning](https://ankitlab.co/projects/)
- [A Probabilistic Theory of Deep Learning](https://arxiv.org/abs/1504.00641)
- [A Probabilistic Framework for Deep Learning](https://papers.nips.cc/paper/6231-a-probabilistic-framework-for-deep-learning.pdf)
- [Lightweight Probabilistic Deep Networks](https://zpascal.net/cvpr2018/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.pdf)
- [Deep Probabilistic Programming](https://arxiv.org/abs/1701.03757)
- https://github.com/oxmlcs/ML_bazaar/wiki/Deep-Learning-and-Probabilistic-Inference
- https://eng.uber.com/pyro/
- [Probabilistic Deep Learning with Python](https://www.manning.com/books/probabilistic-deep-learning-with-python)
- https://livebook.manning.com/book/probabilistic-deep-learning/
- http://csml.stats.ox.ac.uk/
- https://fcai.fi/agile-probabilistic
- http://bayesiandeeplearning.org/2017/papers/59.pdf
- [GluonTS: Probabilistic Time Series Models in Python](https://arxiv.org/abs/1906.05264v2)
- [CS 731: Advanced methods in artificial intelligence, with biomedical applications (Fall 2009)](http://pages.cs.wisc.edu/~dpage/cs731/)
- [CS 838 (Spring 2004): Statistical Relational Learning](https://www.biostat.wisc.edu/~page/838.html)
- https://www.ida.liu.se/~ulfni53/lpp/bok/bok.pdf
- https://www.biostat.wisc.edu/bmi576/
- http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html

### Bayesian Deep Learning

[The abstract of Bayesian Deep learning](http://bayesiandeeplearning.org/) put that:

> While deep learning has been revolutionary for machine learning, most modern deep learning models cannot represent their uncertainty nor take advantage of the well studied tools of probability theory. This has started to change following recent developments of tools and techniques combining Bayesian approaches with deep learning. The intersection of the two fields has received great interest from the community over the past few years, with the introduction of new deep learning models that take advantage of Bayesian techniques, as well as Bayesian models that incorporate deep learning elements [1-11]. In fact, the use of Bayesian techniques in deep learning can be traced back to the 1990s’, in seminal works by Radford Neal [12], David MacKay [13], and Dayan et al. [14]. These gave us tools to reason about deep models’ confidence, and achieved state-of-the-art performance on many tasks. However earlier tools did not adapt when new needs arose (such as scalability to big data), and were consequently forgotten. Such ideas are now being revisited in light of new advances in the field, yielding many exciting new results
> Extending on last year’s workshop’s success, this workshop will again study the advantages and disadvantages of such ideas, and will be a platform to host the recent flourish of ideas using Bayesian approaches in deep learning and using deep learning tools in Bayesian modelling. The program includes a mix of invited talks, contributed talks, and contributed posters. It will be composed of five themes: deep generative models, variational inference using neural network recognition models, practical approximate inference techniques in Bayesian neural networks, applications of Bayesian neural networks, and information theory in deep learning. Future directions for the field will be debated in a panel discussion.
> This year’s main theme will focus on applications of Bayesian deep learning within machine learning and outside of it.

1. Kingma, DP and Welling, M, "Auto-encoding variational Bayes", 2013.
2. Rezende, D, Mohamed, S, and Wierstra, D, "Stochastic backpropagation and approximate inference in deep generative models", 2014.
3. Blundell, C, Cornebise, J, Kavukcuoglu, K, and Wierstra, D, "Weight uncertainty in neural network", 2015.
4. Hernandez-Lobato, JM and Adams, R, "Probabilistic backpropagation for scalable learning of Bayesian neural networks", 2015.
5. Gal, Y and Ghahramani, Z, "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", 2015.
6. Gal, Y and Ghahramani, G, "Bayesian convolutional neural networks with Bernoulli approximate variational inference", 2015.
7. Kingma, D, Salimans, T, and Welling, M. "Variational dropout and the local reparameterization trick", 2015.
8. Balan, AK, Rathod, V, Murphy, KP, and Welling, M, "Bayesian dark knowledge", 2015.
9. Louizos, C and Welling, M, “Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors”, 2016.
10. Lawrence, ND and Quinonero-Candela, J, “Local distance preservation in the GP-LVM through back constraints”, 2006.
11. Tran, D, Ranganath, R, and Blei, DM, “Variational Gaussian Process”, 2015.
12. Neal, R, "Bayesian Learning for Neural Networks", 1996.
13. MacKay, D, "A practical Bayesian framework for backpropagation networks", 1992.
14. Dayan, P, Hinton, G, Neal, R, and Zemel, S, "The Helmholtz machine", 1995.
15. Wilson, AG, Hu, Z, Salakhutdinov, R, and Xing, EP, “Deep Kernel Learning”, 2016.
16. Saatchi, Y and Wilson, AG, “Bayesian GAN”, 2017.
17. MacKay, D.J.C. “Bayesian Methods for Adaptive Models”, PhD thesis, 1992.

***

* [Towards Bayesian Deep Learning: A Framework and Some Existing Methods](https://arxiv.org/abs/1608.06884)
* http://www.wanghao.in/mis.html
* https://github.com/junlulocky/bayesian-deep-learning-notes
* https://github.com/robi56/awesome-bayesian-deep-learning
* https://alexgkendall.com/computer_vision/phd_thesis/
* http://bayesiandeeplearning.org/
* https://ericmjl.github.io/bayesian-deep-learning-demystified/
* http://www.cs.ox.ac.uk/people/yarin.gal/website/blog.html
* http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/
* https://uvadlc.github.io/lectures/apr2019/lecture9-bayesiandeeplearning.pdf


## Statistics and Deep Learning

[A History of Deep Learning](https://www.import.io/post/history-of-deep-learning/)
>Mathematician Ivakhnenko and associates including Lapa arguably created the first working deep learning networks in 1965,
>applying what had been only theories and ideas up to that point.
>
>Ivakhnenko developed the Group Method of Data Handling (GMDH) –
>defined as a “family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets
>that features fully automatic structural and parametric optimization of models” –
>and applied it to neural networks.
>
>For that reason alone, many consider Ivakhnenko the father of modern deep learning.
>
>His learning algorithms used deep feedforward multilayer perceptrons using statistical methods at each layer to find the best features and forward them through the system.
>
>Using GMDH, Ivakhnenko was able to create an 8-layer deep network in 1971,
>and he successfully demonstrated the learning process in a computer identification system called Alpha.

- https://zhuanlan.zhihu.com/p/36519666
- https://wwwhome.ewi.utwente.nl/~schmidtaj/
- http://csml.stats.ox.ac.uk/people/teh/
- http://www.sdlcv-workshop.com/
- https://gkunapuli.github.io/files/17rrbmILP-longslides.pdf
- https://arxiv.org/abs/1810.07132
- https://dashee87.github.io/
- http://lear.inrialpes.fr/workshop/osl2015/
- http://www.stats.ox.ac.uk/~teh/
- http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/
- http://blog.shakirm.com/wp-content/uploads/2015/07/SVDL.pdf
- https://www.ijcai.org/Proceedings/2019/0789.pdf
- http://www.stat.ucla.edu/~jxie/ 
- https://mifods.mit.edu/seminar.php
- https://johanneslederer.com/people/
- https://www.tsu.ge/data/file_db/faculty_zust_sabunebismetk/WEB%20updated%205.05.15-announcement.pdf
- [On Statistical Thinking in Deep Learning: A Talk](http://www.stats.ox.ac.uk/~teh/research/jsm2019/OnStatisticalThinkinginDeepLearning.pdf)
- [On Statistical Thinking in Deep Learning: A Blog Post](http://bulletin.imstat.org/wp-content/uploads/ml-LONG_On_Statistical_Thinking_in_Deep_Learning.pdf)
- [Implementing Bayesian Inference with Neural Networks](http://ul.qucosa.de/api/qucosa%3A34703/attachment/ATT-0/)

### Statistical Relational AI

Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. 
[Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty](https://www.cs.umd.edu/srl-book/)
while incorporating tools from logic, databases, and programming languages to represent structure. 
In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data.

- [Statistical Relational AI Meets Deep Learning](https://gkunapuli.github.io/files/17rrbmILP-longslides.pdf)
- https://people.cs.kuleuven.be/~luc.deraedt/salvador.pdf
- http://www.starai.org/2020/
- https://homes.cs.washington.edu/~pedrod/cikm13.html
- https://www.cs.umd.edu/srl-book/
- https://gkunapuli.github.io/
- https://aifrenz.github.io/
- https://ipvs.informatik.uni-stuttgart.de/mlr/spp-wordpress/
- https://personal.utdallas.edu/~sriraam.natarajan/Courses/starai.html
- http://acai2018.unife.it/
- https://www.biostat.wisc.edu/~page/838.html

### Principal Component Neural Networks

[Nonlinear principal component analysis (NLPCA) is commonly seen as a nonlinear generalization of standard principal component analysis (PCA). It generalizes the principal components from straight lines to curves (nonlinear). Thus, the subspace in the original data space which is described by all nonlinear components is also curved.
Nonlinear PCA can be achieved by using a neural network with an autoassociative architecture also known as autoencoder, replicator network, bottleneck or sandglass type network. Such autoassociative neural network is a multi-layer perceptron that performs an identity mapping, meaning that the output of the network is required to be identical to the input. However, in the middle of the network is a layer that works as a bottleneck in which a reduction of the dimension of the data is enforced. This bottleneck-layer provides the desired component values (scores).](http://www.nlpca.org/)

- http://www.nlpca.org/
- http://users.ics.aalto.fi/~juha/papers/Generalizations_NN_1995.pdf
- https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture18-pca.pdf
- http://www.vision.jhu.edu/teaching/learning/deeplearning19/assets/Baldi_Hornik-89.pdf
- https://www.cs.purdue.edu/homes/dgleich/projects/pca_neural_nets_website/
- https://rdrr.io/cran/caret/man/pcaNNet.html
- http://research.ics.aalto.fi/ica/book/

### Least squares support vector machines

- https://www.esat.kuleuven.be/sista/lssvmlab/
- https://zhenyu-liao.github.io/pdf/journal/LSSVM-TSP.pdf
- https://sci2s.ugr.es/keel/pdf/specific/articulo/vs04.pdf
- https://www.esat.kuleuven.be/sista/members/suykens.html

## Information Theory and Deep Learning

[In short](https://lizhongresearch.miraheze.org/wiki/Understanding_the_Power_of_Neural_Networks), 
Neural Networks extract from the data the most relevant part of the information that describes the statistical dependence between the features and the labels. 
In other words, the size of a Neural Networks specifies a data structure that we can compute and store, 
and the result of training the network is the best approximation of the statistical relationship between the features and the labels 
that can be represented by this data structure.

[In this talk](https://lids.mit.edu/news-and-events/events/information-theoretic-interpretation-deep-neural-networks), 
we formulate a new problem called the "universal feature selection" problem, 
where we need to select from the high dimensional data a low dimensional feature that can be used to solve, not one, but a family of inference problems.
We solve this problem by developing a new information metric that can be used to quantify the semantics of data, and by using a geometric analysis approach. 
We then show that a number of concepts in information theory and statistics such as the HGR correlation and common information are closely connected to the universal feature selection problem. 
At the same time, a number of learning algorithms, PCA, Compressed Sensing, FM, deep neural networks, etc., can also be interpreted as implicitly or explicitly solving the same problem, with various forms of constraints.

* [Universal Features](https://arxiv.org/pdf/1911.09105.pdf)
* https://glouppe.github.io/info8010-deep-learning/
* http://ita.ucsd.edu/
* http://naftali-tishby.mystrikingly.com/
* http://lizhongzheng.mit.edu/
* https://lizhongzheng.mit.edu/sites/default/files/documents/FFSE.pdf
* [Information Theory of Deep Learning](https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/)
* [Anatomize Deep Learning with Information Theory](https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html)
* [“Deep learning - Information theory & Maximum likelihood.”](https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/)
* [Information Theoretic Interpretation of Deep Neural Networks](https://www.rle.mit.edu/sia/wp-content/uploads/2019/07/2019-huang-xu-zheng-wornell.pdf)
- http://pirsa.org/18040050
- https://lizhongresearch.miraheze.org/wiki/Main_Page
- https://lizhongzheng.mit.edu/
- https://lizhongresearch.m.miraheze.org/wiki/Main_Page
- https://www.leiphone.com/news/201703/qzBcOeDYFHtYwgEq.html
- http://nsfcbl.org/
- [Large Margin Deep Neural Networks: Theory and Algorithms](https://arxiv.org/abs/1506.05232v1)
- http://ai.stanford.edu/
- https://www.math.ias.edu/wtdl
- [DEEP 3D REPRESENTATION LEARNING](http://ai.ucsd.edu/~haosu/papers/thesis_finalversion.pdf)
- https://www.mis.mpg.de/ay/index.html
- [Mathematical Algorithms for Artificial Intelligence and Big Data Analysis (Spring 2017)](https://www.math.ucdavis.edu/~strohmer/courses/180BigData/180BigData_info.html)
- https://www.tbsi.edu.cn/index.php?s=/cms/181.html
- https://www.bigr.io/deep-learning-neural-networks-iot/

***
- https://www.ee.ucl.ac.uk/iiml//projects/it_foundations.html
- https://www.isi.edu/~gregv/ijcai/
- https://arxiv.org/abs/1804.09060
- https://people.eng.unimelb.edu.au/jmanton/static/pdf/ISIT2020_preprint.pdf
- http://proceedings.mlr.press/v80/chen18j/chen18j.pdf
- https://arxiv.org/pdf/1503.02406.pdf
- https://stat.mit.edu/calendar/gregory-wornell/
- http://www.mit.edu/~a_makur/publications.html
- https://www.rle.mit.edu/sia/publications/
- https://www.rle.mit.edu/sia/
- https://xiangxiangxu.com/

### Information bottleneck theory

* [On the information bottleneck theory of deep learning](https://www.researchgate.net/publication/325022755_On_the_information_bottleneck_theory_of_deep_learning)
* [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406.pdf)
* https://mc.ai/summary-on-the-information-bottleneck-theory-of-deep-learning/

## Brain Science and AI

Artificial intelligence and brain science have had a swinging relationship of convergence and divergence. 
In the early days of pattern recognition, multi-layer neural networks based on the anatomy and physiology of the visual cortex played a key role, 
but subsequent sophistication of machine learning promoted methods that are little related to the brain. 
Recently, however, the remarkable success of deep neural networks in learning from big data has re-evoked the interests in 
[brain-like artificial intelligence.](http://www.brain-ai.jp/project-outline/)

<img src="http://www.brain-ai.jp/wp-content/uploads/2017/01/brain.png" width="70%" />

- [Theoretical Neuroscience and Deep Learning Theory](http://videolectures.net/deeplearning2017_ganguli_deep_learning_theory/)
- [Bridging Neuroscience and Deep Machine Learning, by building theories that work in the Real World.](https://ankitlab.co/)
* [Center for Mind, Brain, Computation and Technology](https://neuroscience.stanford.edu/mbct/home)
* [Where neuroscience and artificial intelligence converge.](https://braininspired.co/about/)
* https://elsc.huji.ac.il/events/elsc-conference-10
* http://www.brain-ai.jp/organization/
* https://neurodata.io/
* [Artificial Intelligence and brain](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5818638/)
* [Dissecting Artificial Intelligence to Better Understand the Human Brain](https://www.cogneurosociety.org/dissecting-artificial-intelligence-to-better-understand-the-human-brain/)
* [Deep Learning and the Brain](https://quest.mit.edu/projects/)
* [AI and Neuroscience: A virtuous circle](https://deepmind.com/blog/ai-and-neuroscience-virtuous-circle/)
* [Neuroscience-Inspired Artificial Intelligence](http://www.columbia.edu/cu/appliedneuroshp/Papers/out.pdf)
* [深度神经网络（DNN）是否模拟了人类大脑皮层结构？ - Harold Yue的回答 - 知乎](https://www.zhihu.com/question/59800121/answer/184888043)
* [Deep Learning: Branching into brains](https://elifesciences.org/articles/33066)
* https://www.humanbrainproject.eu/en/
* https://www.neuro-central.com/ask-experts-artificial-intelligence-neuroscience/

### Spiking neural networks

- https://zenkelab.org/
- https://neural-reckoning.github.io/snn_workshop_2020/
- https://fzenke.net/
- https://github.com/google/ihmehimmeli
- https://www.humanbrainproject.eu/en/education/participatecollaborate/curriculum/workshops/3rd-curriculum-workshop-ict-snn/
- http://www.cosyne.org/c/index.php?title=Workshops2019_spike_1
- https://2020.wcci-virtual.org/session/workshop-6-design-implementation-and-applications-spiking-neural-networks-and-neuromorphic
- https://niceworkshop.org/nice-2020/nice-2020-tutorials/
- https://github.com/ai-cortex/snn-workshop-amld-2020

### The Thousand Brains Theory of Intelligence

- https://numenta.com/
- https://numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/
- https://www.microsoft.com/en-us/research/uploads/prod/2019/03/42804_The_Thousand_Brains_Theory.pdf
- https://lexfridman.com/jeff-hawkins/


## Cognition Science and Deep Learning

Brain science is the physological theorey of cognitive science, which focus on the physical principle of brain function.
The core problem of cognition science is how to learn in my eyes.

Artificial deep neural networks (DNNs) initially inspired by the brain enable computers to solve cognitive tasks at which humans excel.
In the absence of explanations for such cognitive phenomena, 
[in turn cognitive scientists have started using DNNs as models to investigate biological cognition and its neural basis, creating heated debate.](https://www.sciencedirect.com/science/article/pii/S1364661319300348)

<img src="https://ars.els-cdn.com/content/image/1-s2.0-S1364661319300348-gr2.jpg" width="69%"/>

* https://www.mis.mpg.de/ay/
* [Josh Tenenbaum](http://web.mit.edu/cocosci/josh.html)
* [Deep Neural Networks as Scientific Models](https://www.sciencedirect.com/science/article/pii/S1364661319300348)
* https://wiki.opencog.org/w/Language_learning
* https://github.com/opencog/learn
* [Advancing AI through cognitive science - Spring 2019](https://brendenlake.github.io/AAI-site/)
* [NYU PSYCH-GA 3405.001 / DS-GA 3001.014 : Advancing AI through cognitive science](https://github.com/brendenlake/AAI-site)
* [PSYCH 209: Neural Network Models of Cognition: Principles and Applications](https://web.stanford.edu/class/psych209/)
* [Computational Learning and Memory Group](http://cbl.eng.cam.ac.uk/Public/Lengyel/News)
* [Computational cognitive Science Group @MIT](http://cocosci.mit.edu/)
* [Beyond deep learning](http://beyond-deep-nets.clps.brown.edu/)
* [Cognitive Computation Group @ U. Penn.](https://cogcomp.org/)
* [Computational cognitive modeling](https://brendenlake.github.io/CCM-site/)
* [Mechanisms of geometric cognition](http://hohol.pl/granty/geometry/)
* [Computational Cognitive Science Lab](http://cocosci.princeton.edu/research.php)
* [Deep Learning for Cognitive Computing, Theory](http://www.cs.jyu.fi/ai/vagan/DL4CC.html)
* [TOPIC: FROM ARISTOTLE TO WILLIAM JAMES TO DEEP LEARNING : EVERYTHING OLD IS NEW AGAIN.](https://iccs2019.github.io/#James-Anderson)
* https://www.dcsc.es/
* [Deep Neural Networks as Scientific Models](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930034-8)
* http://vca.ele.tue.nl/
* https://deepmind.com/research/publications/psychlab-psychology-laboratory-deep-reinforcement-learning-agents
* https://www.uni-potsdam.de/en/mlcog/index.html
* https://csai.nl/home/
* https://hadrienj.github.io/about/
* https://iccs2019.github.io/
* https://human-memory.net/
* https://sites.google.com/view/goergen
* https://engineering.purdue.edu/IE/people/ptProfile?resource_id=126302
* https://engineering.columbia.edu/faculty/christos-papadimitriou
* https://www.bio.purdue.edu/People/faculty_dm/directory.php?refID=1000000303
* https://people.csail.mit.edu/mirrokni/Welcome.html
* https://www.mindcogsci.net/
* https://ganguli-gang.stanford.edu/people.html
* http://wiki.ict.usc.edu/cogarch/index.php/Main_Page
* http://cogarch.ict.usc.edu/
* http://bicasociety.org/cogarch/architectures.php

## The lottery ticket hypothesis

[The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training](https://arxiv.org/pdf/1906.02768.pdf)  
by increasing the probability of a “lucky” sub-network initialization being 
present rather than by helping the optimization process (Frankle & Carbin, 2019).

- https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks
- https://arxiv.org/pdf/1905.13405.pdf
- https://arxiv.org/abs/1903.01611
- https://arxiv.org/abs/1905.13405
- https://arxiv.org/abs/1906.02768
- https://arxiv.org/abs/1906.02773
- https://arxiv.org/abs/1909.13458
- https://zhuanlan.zhihu.com/p/84178021
- https://zhuanlan.zhihu.com/p/67782029
- https://openai.com/blog/deep-double-descent/
- https://zhuanlan.zhihu.com/p/100451862


[This project explores the `Lottery Ticket Hypothesis`: the conjecture that neural networks contain much smaller sparse subnetworks capable of training to full accuracy. In the course of this project, we have demonstrated that these subnetworks existed at initialization in small networks and early in training in larger networks. In addition, we have shown that these lottery ticket subnetworks are state-of-the-art pruned neural networks.](https://www.csail.mit.edu/research/lottery-ticket-hypothesis)

- http://www.jfrankle.com/
- https://gkdz.org/#about
- http://yosinski.com/
- [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](https://arxiv.org/abs/1905.01067)
- [OpenLTH: A Framework for Lottery Tickets and Beyond](https://github.com/facebookresearch/open_lth)
- [Code: The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks](https://github.com/google-research/lottery-ticket-hypothesis)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [The Lottery Ticket Hypothesis at Scale](https://deepai.org/publication/the-lottery-ticket-hypothesis-at-scale)
- [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)
- [Linear Mode Connectivity and the Lottery Ticket Hypothesis](https://arxiv.org/abs/1912.05671)
- [The Lottery Ticket Hypothesis for Pre-trained BERT Networks](https://arxiv.org/abs/2007.12223)
- https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/
- https://internetpolicy.mit.edu/neural-networks-and-the-lottery-ticket-hypothesis/
- [Luck Matters: Understanding Training Dynamics of Deep ReLU Networks](https://arxiv.org/abs/1905.13405)

## Double Descent

The model with optimal parameters are not equal to the best model.
$$\fbox{Learning}\not ={Training} \\ Generalization\not ={Optimziation}.$$ 
[Back-propagation (BP), the current de facto training paradigm for deep learning models, is only useful for parameter learning but offers no role in finding an optimal network structure. We need to go beyond BP in order to derive an optimal network, both in structure and in parameter.](https://ee.princeton.edu/people/sun-yuan-kung)

> We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly universal, we don’t yet fully understand why it happens, and view further study of this phenomenon as an important research direction.

<img src="https://openai.com/content/images/size/w1400/2019/12/Frame-1--3-.png" width="70%"/>

- https://arxiv.org/abs/1710.03667
- [Reproducing Deep Double Descent](https://hippocampus-garden.com/double_descent/)
- [Deep Double Descent](https://openai.com/blog/deep-double-descent/)
- [Deep Double Descent (cross-posted on OpenAI blog)](https://windowsontheory.org/2019/12/05/deep-double-descent/)
- [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292)
- [Double Trouble in Double Descent : Bias and Variance(s) in the Lazy Regime](https://arxiv.org/abs/2003.01054)
- https://www.lyrn.ai/
- [High-dimensional dynamics of generalization error in neural networks](https://arxiv.org/abs/1710.03667)
- https://mltheory.org/deep.pdf

## Neural Tangents

- https://arxiv.org/pdf/1905.13192.pdf
- [Neural Tangent Kernel: Convergence and Generlization in Neural Networks](https://neurips.cc/media/Slides/nips/2018/220e(04-15-30)-04-16-50-12604-Neural_Tangent_.pdf)
- https://github.com/google/neural-tangents
- http://simonshaoleidu.com/
- http://www.offconvex.org/2019/10/03/NTK/


